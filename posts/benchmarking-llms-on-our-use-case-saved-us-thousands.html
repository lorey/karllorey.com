<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">How Benchmarking LLMs on Our Use Case Saved Us $1000+ | Karl Lorey</title><meta name="description" content="We benchmarked 300+ models on our actual task and found a cheaper alternative that works just as well." data-next-head=""/><meta property="og:title" content="How Benchmarking LLMs on Our Use Case Saved Us $1000+ | Karl Lorey" data-next-head=""/><meta property="og:description" content="We benchmarked 300+ models on our actual task and found a cheaper alternative that works just as well." data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:url" content="https://karllorey.com/posts/benchmarking-llms-on-our-use-case-saved-us-thousands" data-next-head=""/><meta property="og:site_name" content="Karl Lorey" data-next-head=""/><meta property="og:image" content="https://karllorey.com/social-preview.jpg" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@karllorey" data-next-head=""/><meta name="twitter:title" content="How Benchmarking LLMs on Our Use Case Saved Us $1000+ | Karl Lorey" data-next-head=""/><meta name="twitter:description" content="We benchmarked 300+ models on our actual task and found a cheaper alternative that works just as well." data-next-head=""/><meta name="twitter:image" content="https://karllorey.com/social-preview.jpg" data-next-head=""/><meta property="article:published_time" content="2026-01-19T11:00:00.000Z" data-next-head=""/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/chunks/715447f45171d1c4.css" as="style"/><link rel="stylesheet" href="/_next/static/chunks/715447f45171d1c4.css" data-n-g=""/><noscript data-n-css=""></noscript><script src="/_next/static/chunks/3898cfe4d58617f2.js" defer=""></script><script src="/_next/static/chunks/acf108a5e616596f.js" defer=""></script><script src="/_next/static/chunks/680c60bb94721e7d.js" defer=""></script><script src="/_next/static/chunks/54696b9c9edce1fb.js" defer=""></script><script src="/_next/static/chunks/turbopack-1107b6f9a958f3b9.js" defer=""></script><script src="/_next/static/chunks/b6fd59edaf8533ae.js" defer=""></script><script src="/_next/static/chunks/4c3740c960b0f04c.js" defer=""></script><script src="/_next/static/chunks/turbopack-a231b91cbae4b51c.js" defer=""></script><script src="/_next/static/0N86bJLsr6UTulH7oBajj/_ssgManifest.js" defer=""></script><script src="/_next/static/0N86bJLsr6UTulH7oBajj/_buildManifest.js" defer=""></script></head><body class="antialiased"><div id="__next"><div class="max-w-xl mx-auto p-8"><header class="text-left md:text-center py-8 md:py-16"><div class="title py-2 md:py-4 ml-[-0.05em] md:ml-0"><a href="/">Karl Lorey</a></div><nav class="my-2 md:my-5"><ul class="flex flex-wrap justify-start md:justify-center gap-4"><li><a href="/human">Human</a></li><li><a href="/founder">Founder</a></li><li><a href="/portfolio">Builder</a></li><li><a href="/techie">Techie</a></li><li><a href="/vc">Investor</a></li><li><a href="/blog">Lorey Ipsum</a></li></ul></nav></header><hr class="mb-16"/><main class="content"><h1>How Benchmarking LLMs on Our Use Case Saved Us $1000+</h1><div class="text-gray-400 text-sm mb-8">Jan 19, 2026</div><div class="markdown"><p>This is the story of how I benchmarked LLMs for a friend of mine, saved him thousands in the process, and built a product around it.</p>
<p>Let me first tell you about my friend. He&#x27;s an entrepreneur, and a successful one at that.
After he successfully built his last company, a SaaS business, he&#x27;s now building his next business as a solo founder.
Although his last company was a SaaS startup, he&#x27;s non-technical. While I cannot go into details, he&#x27;s re-thinking a traditional business with AI.
As part of this business, he&#x27;s been deploying a lot of prompts to automate processes that have previously been done by humans.
And for that, he picked GPT-5 because, well, it&#x27;s the default choice.
It&#x27;s solid in most benchmarks, everyone uses it, they never questioned it.</p>
<p>But as adoption grew, so did the API bill. Hundreds of dollars monthly.
As I also do a lot of LLM engineering for my company, he asked me to take a look and help him out.
As part of this process, we benchmarked different LLMs against his use cases.
It quickly became clear that he can swap out ChatGPT with cheaper alternatives for a fraction of the costs, saving him north of $1000 monthly.</p>
<h2>The Problem: Benchmarks don&#x27;t predict performance on your specific task</h2>
<p>When picking an LLM, most people just use choose a model from their favorite provider.
For me, that&#x27;s anthropic, and so depending on the task, I choose Opus, Sonnet, or Haiku.
Same for OpenAI with a slightly higher variance.
If you&#x27;re sophisticated, you might event check the latest benchmark, e.g. Artificial Analysis or LM Arena.
Or whatever is the latest hot benchmark for something related to your use case:
GPQA Diamond, AIME, SWE Bench, MATH 500, Humanity&#x27;s Last Exam, ARC-AGI, MMMLU...</p>
<p>But here&#x27;s the thing:
NONE of these predict performance on YOUR specific task.</p>
<p>A model that tops reasoning benchmarks might be mediocre at damage cost estimation.
Or customer support in your customers&#x27; native tongue.
Or spatial reasoning in Germany.
Or data extraction via playwright.
Or whatever you&#x27;re actually building.</p>
<p>The only way to know is to test on your actual prompts.</p>
<h2>Building benchmarks ourselves</h2>
<p>So we built the benchmarks.
We identified the main use cases of prompts, e.g. customer support and cost calculation.
We already had the prompts that were used to do this. Sometimes fully automated, sometimes to generate an answer that would then be refined.
To create a ground truth for all use cases, we collected as many real-life examples as possible.
For customer support, we extracted the actual chats via <a href="https://whapi.cloud/">WHAPI</a> along with the agents (my friends) answer.
Or for cost estimation, we collected all the data we had about the contract along with the human-made estimate that got sent out to the customer.
If you know a specific model is good enough, you can also simply use its output.</p>
<p>Now we had the context, the pompts, and the expected outputs.
But how to now score all the LLMs against each other?
For this, we applied the &quot;LLM as a judge&quot; approach.
For all our samples, we used prompt + context to generate an answer.
Then we let a (frontier) LLM judge the result based on the given answer of the model we benchmarked.
This then gave us a score for every LLM.
We also checked the results to make sure they aligned with our subjective scoring.</p>
<h2>Deciding on the best model</h2>
<p>Now that we had a score to measure quality per LLM, the question was:
Which model should we choose?
In practice, you want a model that provides a balance of quality, cost, and latency.
For our customer support case, latency was important. We couldn&#x27;t wait for GPT-5 that took up to a minute with enough context.
For price estimation, we wanted the results to be as good as possible for a reasonable price.</p>
<p>This made us realize, we needed to measure both cost and latency.</p>
<ul>
<li>Cost: For cost, we quickly realized that simply comparing token costs is not enough:
Since response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,
we decided to simply measure overall cost per answer and average per use case or benchmark.</li>
<li>Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.
Of course, that differs for chat applications where time to first token, etc. can be essential UX, too.</li>
</ul>
<p>This finally gave us a list of models per use case with price, cost, and latency.
To decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.</p>
<p>In theory, there&#x27;s a concept called Pareto Efficiency that can be applied:
For the formal definition, Wikipedia sure does a better job than me.
For the informal definition, here&#x27;s my take:</p>
<blockquote>
<p>Given that you have a benchmark across 100 LLMs with a cost and score (let&#x27;s forget latency for a second).
There&#x27;s no point in comparing all 100 LLMs.
For most LLMs, you will find a model that cheaper AND better.
This means there&#x27;s no point in looking at it, as you have one that&#x27;s better in both dimensions.
Doing this for all LLMs in a benchmark, you get a pareto frontier:
The best LLM for a given price.
The higher the price, the more quality you get.</p>
</blockquote>
<h2>Saving $1000 monthly by switching the models</h2>
<p>With the described method, we found a significantly chepaer model for both use cases.
Across the two use cases, this saved him roughly 80% of the monthly costs and over $1000.</p>
<p>Since no technical post ends without a small plug,
in the following section, I will tell you that I launched this as a small tool.
Stop here if you&#x27;re not interested.</p>
<h2>evalry: a tool to benchmark your usecase across 300+ LLMs</h2>
<p>As you see, benchmarking and truly finding an optimal model is more complex than we initially though.
That&#x27;s why my friend never did it, that why I unsually don&#x27;t do it.
You need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.
Manually testing even 5 models can quickly become a multi-hour endeavor.
And new models drop weekly. Keeping up is impossible.
The same model in a month? Half the price because some LLM wizard like Simon Bohm dropped inference costs in half.</p>
<p>So to help my friend and anyone else with the same problem, I built <a href="https://evalry.com">Evalry</a>.
It provides all this in one simple tool that does the heavy lifting for you:
It tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.
I&#x27;m also planning to set up continuous monitoring. So when a better model appears, that&#x27;s cheaper, faster, or higher quality for your use case, you get notified.</p>
<p>So if you&#x27;re paying for LLM APIs and have never tested alternatives on your actual prompts, you&#x27;re likely overpaying.
Give <a href="https://evalry.com">Evalry</a> a try. It takes 5 minutes to find out if there&#x27;s a better model for your use case.
Or if you&#x27;re short on time, find the model you&#x27;re currently using and try the five models that have similar performance on average.</p></div></main><hr class="my-16"/><footer class="text-center text-sm py-10"><p>Karl Lorey: founder, builder, investor living in Germany.<br/>Reach me at mail (at) karllorey (dot) com or find me here:</p><p class="flex justify-center gap-4 flex-wrap text-lg mt-4"><a href="https://github.com/lorey" target="_blank" rel="noopener noreferrer" title="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://www.linkedin.com/in/karllorey" target="_blank" rel="noopener noreferrer" title="LinkedIn"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://x.com/karllorey" target="_blank" rel="noopener noreferrer" title="X"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a><a href="https://bsky.app/profile/karllorey.bsky.social" target="_blank" rel="noopener noreferrer" title="Bluesky"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"></path></svg></a><a href="https://www.crunchbase.com/person/karl-lorey" target="_blank" rel="noopener noreferrer" title="Crunchbase"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M234.5 5.7c13.9-5 29.1-5 43.1 0l192 68.6C495 83.4 512 107.5 512 134.6l0 242.9c0 27-17 51.2-42.5 60.3l-192 68.6c-13.9 5-29.1 5-43.1 0l-192-68.6C17 428.6 0 404.5 0 377.4L0 134.6c0-27 17-51.2 42.5-60.3l192-68.6zM256 66L82.3 128 256 190l173.7-62L256 66zm32 368.6l160-57.1 0-188L288 246.6l0 188z"></path></svg></a><a href="https://www.producthunt.com/@karllorey" target="_blank" rel="noopener noreferrer" title="Product Hunt"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M326.3 218.8c0 20.5-16.7 37.2-37.2 37.2h-70.3v-74.4h70.3c20.5 0 37.2 16.7 37.2 37.2zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-128.1-37.2c0-47.9-38.9-86.8-86.8-86.8H169.2v248h49.6v-74.4h70.3c47.9 0 86.8-38.9 86.8-86.8z"></path></svg></a><a href="https://angel.co/karllorey" target="_blank" rel="noopener noreferrer" title="AngelList"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M347.1 215.4c11.7-32.6 45.4-126.9 45.4-157.1 0-26.6-15.7-48.9-43.7-48.9-44.6 0-84.6 131.7-97.1 163.1C242 144 196.6 0 156.6 0c-31.1 0-45.7 22.9-45.7 51.7 0 35.3 34.2 126.8 46.6 162-6.3-2.3-13.1-4.3-20-4.3-23.4 0-48.3 29.1-48.3 52.6 0 8.9 4.9 21.4 8 29.7-36.9 10-51.1 34.6-51.1 71.7C46 435.6 114.4 512 210.6 512c118 0 191.4-88.6 191.4-202.9 0-43.1-6.9-82-54.9-93.7zM311.7 108c4-12.3 21.1-64.3 37.1-64.3 8.6 0 10.9 8.9 10.9 16 0 19.1-38.6 124.6-47.1 148l-34-6 33.1-93.7zM142.3 48.3c0-11.9 14.5-45.7 46.3 47.1l34.6 100.3c-15.6-1.3-27.7-3-35.4 1.4-10.9-28.8-45.5-119.7-45.5-148.8zM140 244c29.3 0 67.1 94.6 67.1 107.4 0 5.1-4.9 11.4-10.6 11.4-20.9 0-76.9-76.9-76.9-97.7.1-7.7 12.7-21.1 20.4-21.1zm184.3 186.3c-29.1 32-66.3 48.6-109.7 48.6-59.4 0-106.3-32.6-128.9-88.3-17.1-43.4 3.8-68.3 20.6-68.3 11.4 0 54.3 60.3 54.3 73.1 0 4.9-7.7 8.3-11.7 8.3-16.1 0-22.4-15.5-51.1-51.4-29.7 29.7 20.5 86.9 58.3 86.9 26.1 0 43.1-24.2 38-42 3.7 0 8.3.3 11.7-.6 1.1 27.1 9.1 59.4 41.7 61.7 0-.9 2-7.1 2-7.4 0-17.4-10.6-32.6-10.6-50.3 0-28.3 21.7-55.7 43.7-71.7 8-6 17.7-9.7 27.1-13.1 9.7-3.7 20-8 27.4-15.4-1.1-11.2-5.7-21.1-16.9-21.1-27.7 0-120.6 4-120.6-39.7 0-6.7.1-13.1 17.4-13.1 32.3 0 114.3 8 138.3 29.1 18.1 16.1 24.3 113.2-31 174.7zm-98.6-126c9.7 3.1 19.7 4 29.7 6-7.4 5.4-14 12-20.3 19.1-2.8-8.5-6.2-16.8-9.4-25.1z"></path></svg></a><a href="https://gitlab.com/lorey" target="_blank" rel="noopener noreferrer" title="GitLab"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M503.5 204.6L502.8 202.8L433.1 21.02C431.7 17.45 429.2 14.43 425.9 12.38C423.5 10.83 420.8 9.865 417.9 9.57C415 9.275 412.2 9.653 409.5 10.68C406.8 11.7 404.4 13.34 402.4 15.46C400.5 17.58 399.1 20.13 398.3 22.9L351.3 166.9H160.8L113.7 22.9C112.9 20.13 111.5 17.59 109.6 15.47C107.6 13.35 105.2 11.72 102.5 10.7C99.86 9.675 96.98 9.295 94.12 9.587C91.26 9.878 88.51 10.83 86.08 12.38C82.84 14.43 80.33 17.45 78.92 21.02L9.267 202.8L8.543 204.6C-1.484 230.8-2.72 259.6 5.023 286.6C12.77 313.5 29.07 337.3 51.47 354.2L51.74 354.4L52.33 354.8L158.3 434.3L210.9 474L242.9 498.2C246.6 500.1 251.2 502.5 255.9 502.5C260.6 502.5 265.2 500.1 268.9 498.2L300.9 474L353.5 434.3L460.2 354.4L460.5 354.1C482.9 337.2 499.2 313.5 506.1 286.6C514.7 259.6 513.5 230.8 503.5 204.6z"></path></svg></a><a href="https://www.goodreads.com/karllorey" target="_blank" rel="noopener noreferrer" title="Goodreads"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M299.9 191.2c5.1 37.3-4.7 79-35.9 100.7-22.3 15.5-52.8 14.1-70.8 5.7-37.1-17.3-49.5-58.6-46.8-97.2 4.3-60.9 40.9-87.9 75.3-87.5 46.9-.2 71.8 31.8 78.2 78.3zM448 88v336c0 30.9-25.1 56-56 56H56c-30.9 0-56-25.1-56-56V88c0-30.9 25.1-56 56-56h336c30.9 0 56 25.1 56 56zM330 313.2s-.1-34-.1-217.3h-29v40.3c-.8.3-1.2-.5-1.6-1.2-9.6-20.7-35.9-46.3-76-46-51.9.4-87.2 31.2-100.6 77.8-4.3 14.9-5.8 30.1-5.5 45.6 1.7 77.9 45.1 117.8 112.4 115.2 28.9-1.1 54.5-17 69-45.2.5-1 1.1-1.9 1.7-2.9.2.1.4.1.6.2.3 3.8.2 30.7.1 34.5-.2 14.8-2 29.5-7.2 43.5-7.8 21-22.3 34.7-44.5 39.5-17.8 3.9-35.6 3.8-53.2-1.2-21.5-6.1-36.5-19-41.1-41.8-.3-1.6-1.3-1.3-2.3-1.3h-26.8c.8 10.6 3.2 20.3 8.5 29.2 24.2 40.5 82.7 48.5 128.2 37.4 49.9-12.3 67.3-54.9 67.4-106.3z"></path></svg></a><a href="https://www.instagram.com/karllorey" target="_blank" rel="noopener noreferrer" title="Instagram"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"></path></svg></a><a href="https://medium.com/@karllorey" target="_blank" rel="noopener noreferrer" title="Medium"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M180.5,74.262C80.813,74.262,0,155.633,0,256S80.819,437.738,180.5,437.738,361,356.373,361,256,280.191,74.262,180.5,74.262Zm288.25,10.646c-49.845,0-90.245,76.619-90.245,171.095s40.406,171.1,90.251,171.1,90.251-76.619,90.251-171.1H559C559,161.5,518.6,84.908,468.752,84.908Zm139.506,17.821c-17.526,0-31.735,68.628-31.735,153.274s14.2,153.274,31.735,153.274S640,340.631,640,256C640,171.351,625.785,102.729,608.258,102.729Z"></path></svg></a><a href="https://substack.com/@karllorey" target="_blank" rel="noopener noreferrer" title="Substack"><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M22.539 8.242H1.46V5.406h21.08v2.836zM1.46 10.812V24L12 18.11 22.54 24V10.812H1.46zM22.54 0H1.46v2.836h21.08V0z"></path></svg></a><a href="https://www.meetup.com/members/196097665/" target="_blank" rel="noopener noreferrer" title="Meetup"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M99 414.3c1.1 5.7-2.3 11.1-8 12.3-5.4 1.1-10.9-2.3-12-8-1.1-5.4 2.3-11.1 7.7-12.3 5.4-1.2 11.1 2.3 12.3 8zm143.1 71.4c-6.3 4.6-8 13.4-3.7 20 4.6 6.6 13.4 8.3 20 3.7 6.3-4.6 8-13.4 3.4-20-4.2-6.5-13.1-8.3-19.7-3.7zm-86-462.3c6.3-1.4 10.3-7.7 8.9-14-1.1-6.6-7.4-10.6-13.7-9.1-6.3 1.4-10.3 7.7-9.1 14 1.4 6.6 7.6 10.6 13.9 9.1zM34.4 226.3c-10-6.9-23.7-4.3-30.6 6-6.9 10-4.3 24 5.7 30.9 10 7.1 23.7 4.6 30.6-5.7 6.9-10.4 4.3-24.1-5.7-31.2zm272-170.9c10.6-6.3 13.7-20 7.7-30.3-6.3-10.6-19.7-14-30-7.7s-13.7 20-7.4 30.6c6 10.3 19.4 13.7 29.7 7.4zm-191.1 58c7.7-5.4 9.4-16 4.3-23.7s-15.7-9.4-23.1-4.3c-7.7 5.4-9.4 16-4.3 23.7 5.1 7.8 15.6 9.5 23.1 4.3zm372.3 156c-7.4 1.7-12.3 9.1-10.6 16.9 1.4 7.4 8.9 12.3 16.3 10.6 7.4-1.4 12.3-8.9 10.6-16.6-1.5-7.4-8.9-12.3-16.3-10.9zm39.7-56.8c-1.1-5.7-6.6-9.1-12-8-5.7 1.1-9.1 6.9-8 12.6 1.1 5.4 6.6 9.1 12.3 8 5.4-1.5 9.1-6.9 7.7-12.6zM447 138.9c-8.6 6-10.6 17.7-4.9 26.3 5.7 8.6 17.4 10.6 26 4.9 8.3-6 10.3-17.7 4.6-26.3-5.7-8.7-17.4-10.9-25.7-4.9zm-6.3 139.4c26.3 43.1 15.1 100-26.3 129.1-17.4 12.3-37.1 17.7-56.9 17.1-12 47.1-69.4 64.6-105.1 32.6-1.1.9-2.6 1.7-3.7 2.9-39.1 27.1-92.3 17.4-119.4-22.3-9.7-14.3-14.6-30.6-15.1-46.9-65.4-10.9-90-94-41.1-139.7-28.3-46.9.6-107.4 53.4-114.9C151.6 70 234.1 38.6 290.1 82c67.4-22.3 136.3 29.4 130.9 101.1 41.1 12.6 52.8 66.9 19.7 95.2zm-70 74.3c-3.1-20.6-40.9-4.6-43.1-27.1-3.1-32 43.7-101.1 40-128-3.4-24-19.4-29.1-33.4-29.4-13.4-.3-16.9 2-21.4 4.6-2.9 1.7-6.6 4.9-11.7-.3-6.3-6-11.1-11.7-19.4-12.9-12.3-2-17.7 2-26.6 9.7-3.4 2.9-12 12.9-20 9.1-3.4-1.7-15.4-7.7-24-11.4-16.3-7.1-40 4.6-48.6 20-12.9 22.9-38 113.1-41.7 125.1-8.6 26.6 10.9 48.6 36.9 47.1 11.1-.6 18.3-4.6 25.4-17.4 4-7.4 41.7-107.7 44.6-112.6 2-3.4 8.9-8 14.6-5.1 5.7 3.1 6.9 9.4 6 15.1-1.1 9.7-28 70.9-28.9 77.7-3.4 22.9 26.9 26.6 38.6 4 3.7-7.1 45.7-92.6 49.4-98.3 4.3-6.3 7.4-8.3 11.7-8 3.1 0 8.3.9 7.1 10.9-1.4 9.4-35.1 72.3-38.9 87.7-4.6 20.6 6.6 41.4 24.9 50.6 11.4 5.7 62.5 15.7 58.5-11.1zm5.7 92.3c-10.3 7.4-12.9 22-5.7 32.6 7.1 10.6 21.4 13.1 32 6 10.6-7.4 13.1-22 6-32.6-7.4-10.6-21.7-13.5-32.3-6z"></path></svg></a><a href="https://www.researchgate.net/profile/Karl_Lorey" target="_blank" rel="noopener noreferrer" title="ResearchGate"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 32v448h448V32H0zm262.2 334.4c-6.6 3-33.2 6-50-14.2-9.2-10.6-25.3-33.3-42.2-63.6-8.9 0-14.7 0-21.4-.6v46.4c0 23.5 6 21.2 25.8 23.9v8.1c-6.9-.3-23.1-.8-35.6-.8-13.1 0-26.1.6-33.6.8v-8.1c15.5-2.9 22-1.3 22-23.9V225c0-22.6-6.4-21-22-23.9V193c25.8 1 53.1-.6 70.9-.6 31.7 0 55.9 14.4 55.9 45.6 0 21.1-16.7 42.2-39.2 47.5 13.6 24.2 30 45.6 42.2 58.9 7.2 7.8 17.2 14.7 27.2 14.7v7.3zm22.9-135c-23.3 0-32.2-15.7-32.2-32.2V167c0-12.2 8.8-30.4 34-30.4s30.4 17.9 30.4 17.9l-10.7 7.2s-5.5-12.5-19.7-12.5c-7.9 0-19.7 7.3-19.7 19.7v26.8c0 13.4 6.6 23.3 17.9 23.3 14.1 0 21.5-10.9 21.5-26.8h-17.9v-10.7h30.4c0 20.5 4.7 49.9-34 49.9zm-116.5 44.7c-9.4 0-13.6-.3-20-.8v-69.7c6.4-.6 15-.6 22.5-.6 23.3 0 37.2 12.2 37.2 34.5 0 21.9-15 36.6-39.7 36.6z"></path></svg></a></p><div class="mt-10 space-y-2"><p class="pb-0"><a href="/legal">Legal</a> · <a href="/privacy">Privacy</a> · <a href="https://github.com/lorey/karllorey.com">Source</a></p><p class="pb-0">© 2025 Karl Lorey.<!-- --> <!-- -->Updated<!-- --> <!-- -->Jan 20, 2026<!-- --> <!-- -->(<!-- -->96249ca<!-- -->)</p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"How Benchmarking LLMs on Our Use Case Saved Us $1000+","slug":"benchmarking-llms-on-our-use-case-saved-us-thousands","date":"2026-01-19T11:00:00.000Z","tags":"LLM, AI, Tools, Startups","category":"Projects","description":"We benchmarked 300+ models on our actual task and found a cheaper alternative that works just as well.","type":"text","status":"published","content":"\nThis is the story of how I benchmarked LLMs for a friend of mine, saved him thousands in the process, and built a product around it.\n\nLet me first tell you about my friend. He's an entrepreneur, and a successful one at that.\nAfter he successfully built his last company, a SaaS business, he's now building his next business as a solo founder.\nAlthough his last company was a SaaS startup, he's non-technical. While I cannot go into details, he's re-thinking a traditional business with AI.\nAs part of this business, he's been deploying a lot of prompts to automate processes that have previously been done by humans.\nAnd for that, he picked GPT-5 because, well, it's the default choice. \nIt's solid in most benchmarks, everyone uses it, they never questioned it.\n\nBut as adoption grew, so did the API bill. Hundreds of dollars monthly.\nAs I also do a lot of LLM engineering for my company, he asked me to take a look and help him out.\nAs part of this process, we benchmarked different LLMs against his use cases.\nIt quickly became clear that he can swap out ChatGPT with cheaper alternatives for a fraction of the costs, saving him north of $1000 monthly.\n\n## The Problem: Benchmarks don't predict performance on your specific task\n\nWhen picking an LLM, most people just use choose a model from their favorite provider. \nFor me, that's anthropic, and so depending on the task, I choose Opus, Sonnet, or Haiku.\nSame for OpenAI with a slightly higher variance.\nIf you're sophisticated, you might event check the latest benchmark, e.g. Artificial Analysis or LM Arena. \nOr whatever is the latest hot benchmark for something related to your use case:\nGPQA Diamond, AIME, SWE Bench, MATH 500, Humanity's Last Exam, ARC-AGI, MMMLU...\n\nBut here's the thing: \nNONE of these predict performance on YOUR specific task.\n\nA model that tops reasoning benchmarks might be mediocre at damage cost estimation. \nOr customer support in your customers' native tongue. \nOr spatial reasoning in Germany.\nOr data extraction via playwright. \nOr whatever you're actually building.\n\nThe only way to know is to test on your actual prompts.\n\n## Building benchmarks ourselves\n\nSo we built the benchmarks.\nWe identified the main use cases of prompts, e.g. customer support and cost calculation.\nWe already had the prompts that were used to do this. Sometimes fully automated, sometimes to generate an answer that would then be refined.\nTo create a ground truth for all use cases, we collected as many real-life examples as possible.\nFor customer support, we extracted the actual chats via [WHAPI](https://whapi.cloud/) along with the agents (my friends) answer.\nOr for cost estimation, we collected all the data we had about the contract along with the human-made estimate that got sent out to the customer.\nIf you know a specific model is good enough, you can also simply use its output.\n\nNow we had the context, the pompts, and the expected outputs. \nBut how to now score all the LLMs against each other?\nFor this, we applied the \"LLM as a judge\" approach.\nFor all our samples, we used prompt + context to generate an answer.\nThen we let a (frontier) LLM judge the result based on the given answer of the model we benchmarked.\nThis then gave us a score for every LLM.\nWe also checked the results to make sure they aligned with our subjective scoring.\n\n## Deciding on the best model\n\nNow that we had a score to measure quality per LLM, the question was:\nWhich model should we choose?\nIn practice, you want a model that provides a balance of quality, cost, and latency.\nFor our customer support case, latency was important. We couldn't wait for GPT-5 that took up to a minute with enough context.\nFor price estimation, we wanted the results to be as good as possible for a reasonable price.\n\nThis made us realize, we needed to measure both cost and latency.\n- Cost: For cost, we quickly realized that simply comparing token costs is not enough:\n  Since response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,\n  we decided to simply measure overall cost per answer and average per use case or benchmark.\n- Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.\n  Of course, that differs for chat applications where time to first token, etc. can be essential UX, too.\n\nThis finally gave us a list of models per use case with price, cost, and latency.\nTo decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.\n\nIn theory, there's a concept called Pareto Efficiency that can be applied:\nFor the formal definition, Wikipedia sure does a better job than me.\nFor the informal definition, here's my take:\n\n\u003e Given that you have a benchmark across 100 LLMs with a cost and score (let's forget latency for a second).\n  There's no point in comparing all 100 LLMs.\n  For most LLMs, you will find a model that cheaper AND better.\n  This means there's no point in looking at it, as you have one that's better in both dimensions.\n  Doing this for all LLMs in a benchmark, you get a pareto frontier:\n  The best LLM for a given price.\n  The higher the price, the more quality you get.\n\n## Saving $1000 monthly by switching the models\n\nWith the described method, we found a significantly chepaer model for both use cases.\nAcross the two use cases, this saved him roughly 80% of the monthly costs and over $1000.\n\nSince no technical post ends without a small plug,\nin the following section, I will tell you that I launched this as a small tool.\nStop here if you're not interested.\n\n## evalry: a tool to benchmark your usecase across 300+ LLMs\n\nAs you see, benchmarking and truly finding an optimal model is more complex than we initially though.\nThat's why my friend never did it, that why I unsually don't do it.\nYou need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.\nManually testing even 5 models can quickly become a multi-hour endeavor.\nAnd new models drop weekly. Keeping up is impossible.\nThe same model in a month? Half the price because some LLM wizard like Simon Bohm dropped inference costs in half.\n\nSo to help my friend and anyone else with the same problem, I built [Evalry](https://evalry.com).\nIt provides all this in one simple tool that does the heavy lifting for you:\nIt tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.\nI'm also planning to set up continuous monitoring. So when a better model appears, that's cheaper, faster, or higher quality for your use case, you get notified.\n\nSo if you're paying for LLM APIs and have never tested alternatives on your actual prompts, you're likely overpaying.\nGive [Evalry](https://evalry.com) a try. It takes 5 minutes to find out if there's a better model for your use case.\nOr if you're short on time, find the model you're currently using and try the five models that have similar performance on average.\n"},"mdxSource":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    h2: \"h2\",\n    li: \"li\",\n    p: \"p\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"This is the story of how I benchmarked LLMs for a friend of mine, saved him thousands in the process, and built a product around it.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Let me first tell you about my friend. He's an entrepreneur, and a successful one at that.\\nAfter he successfully built his last company, a SaaS business, he's now building his next business as a solo founder.\\nAlthough his last company was a SaaS startup, he's non-technical. While I cannot go into details, he's re-thinking a traditional business with AI.\\nAs part of this business, he's been deploying a lot of prompts to automate processes that have previously been done by humans.\\nAnd for that, he picked GPT-5 because, well, it's the default choice.\\nIt's solid in most benchmarks, everyone uses it, they never questioned it.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But as adoption grew, so did the API bill. Hundreds of dollars monthly.\\nAs I also do a lot of LLM engineering for my company, he asked me to take a look and help him out.\\nAs part of this process, we benchmarked different LLMs against his use cases.\\nIt quickly became clear that he can swap out ChatGPT with cheaper alternatives for a fraction of the costs, saving him north of $1000 monthly.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The Problem: Benchmarks don't predict performance on your specific task\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When picking an LLM, most people just use choose a model from their favorite provider.\\nFor me, that's anthropic, and so depending on the task, I choose Opus, Sonnet, or Haiku.\\nSame for OpenAI with a slightly higher variance.\\nIf you're sophisticated, you might event check the latest benchmark, e.g. Artificial Analysis or LM Arena.\\nOr whatever is the latest hot benchmark for something related to your use case:\\nGPQA Diamond, AIME, SWE Bench, MATH 500, Humanity's Last Exam, ARC-AGI, MMMLU...\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But here's the thing:\\nNONE of these predict performance on YOUR specific task.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A model that tops reasoning benchmarks might be mediocre at damage cost estimation.\\nOr customer support in your customers' native tongue.\\nOr spatial reasoning in Germany.\\nOr data extraction via playwright.\\nOr whatever you're actually building.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The only way to know is to test on your actual prompts.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Building benchmarks ourselves\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So we built the benchmarks.\\nWe identified the main use cases of prompts, e.g. customer support and cost calculation.\\nWe already had the prompts that were used to do this. Sometimes fully automated, sometimes to generate an answer that would then be refined.\\nTo create a ground truth for all use cases, we collected as many real-life examples as possible.\\nFor customer support, we extracted the actual chats via \", _jsx(_components.a, {\n        href: \"https://whapi.cloud/\",\n        children: \"WHAPI\"\n      }), \" along with the agents (my friends) answer.\\nOr for cost estimation, we collected all the data we had about the contract along with the human-made estimate that got sent out to the customer.\\nIf you know a specific model is good enough, you can also simply use its output.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now we had the context, the pompts, and the expected outputs.\\nBut how to now score all the LLMs against each other?\\nFor this, we applied the \\\"LLM as a judge\\\" approach.\\nFor all our samples, we used prompt + context to generate an answer.\\nThen we let a (frontier) LLM judge the result based on the given answer of the model we benchmarked.\\nThis then gave us a score for every LLM.\\nWe also checked the results to make sure they aligned with our subjective scoring.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Deciding on the best model\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now that we had a score to measure quality per LLM, the question was:\\nWhich model should we choose?\\nIn practice, you want a model that provides a balance of quality, cost, and latency.\\nFor our customer support case, latency was important. We couldn't wait for GPT-5 that took up to a minute with enough context.\\nFor price estimation, we wanted the results to be as good as possible for a reasonable price.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This made us realize, we needed to measure both cost and latency.\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Cost: For cost, we quickly realized that simply comparing token costs is not enough:\\nSince response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,\\nwe decided to simply measure overall cost per answer and average per use case or benchmark.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.\\nOf course, that differs for chat applications where time to first token, etc. can be essential UX, too.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This finally gave us a list of models per use case with price, cost, and latency.\\nTo decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In theory, there's a concept called Pareto Efficiency that can be applied:\\nFor the formal definition, Wikipedia sure does a better job than me.\\nFor the informal definition, here's my take:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"Given that you have a benchmark across 100 LLMs with a cost and score (let's forget latency for a second).\\nThere's no point in comparing all 100 LLMs.\\nFor most LLMs, you will find a model that cheaper AND better.\\nThis means there's no point in looking at it, as you have one that's better in both dimensions.\\nDoing this for all LLMs in a benchmark, you get a pareto frontier:\\nThe best LLM for a given price.\\nThe higher the price, the more quality you get.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Saving $1000 monthly by switching the models\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With the described method, we found a significantly chepaer model for both use cases.\\nAcross the two use cases, this saved him roughly 80% of the monthly costs and over $1000.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Since no technical post ends without a small plug,\\nin the following section, I will tell you that I launched this as a small tool.\\nStop here if you're not interested.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"evalry: a tool to benchmark your usecase across 300+ LLMs\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"As you see, benchmarking and truly finding an optimal model is more complex than we initially though.\\nThat's why my friend never did it, that why I unsually don't do it.\\nYou need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.\\nManually testing even 5 models can quickly become a multi-hour endeavor.\\nAnd new models drop weekly. Keeping up is impossible.\\nThe same model in a month? Half the price because some LLM wizard like Simon Bohm dropped inference costs in half.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So to help my friend and anyone else with the same problem, I built \", _jsx(_components.a, {\n        href: \"https://evalry.com\",\n        children: \"Evalry\"\n      }), \".\\nIt provides all this in one simple tool that does the heavy lifting for you:\\nIt tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.\\nI'm also planning to set up continuous monitoring. So when a better model appears, that's cheaper, faster, or higher quality for your use case, you get notified.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So if you're paying for LLM APIs and have never tested alternatives on your actual prompts, you're likely overpaying.\\nGive \", _jsx(_components.a, {\n        href: \"https://evalry.com\",\n        children: \"Evalry\"\n      }), \" a try. It takes 5 minutes to find out if there's a better model for your use case.\\nOr if you're short on time, find the model you're currently using and try the five models that have similar performance on average.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"benchmarking-llms-on-our-use-case-saved-us-thousands"},"buildId":"0N86bJLsr6UTulH7oBajj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>