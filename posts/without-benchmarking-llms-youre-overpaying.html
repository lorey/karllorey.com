<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Without Benchmarking LLMs, You&#x27;re Likely Overpaying 5-10x | Karl Lorey</title><meta name="description" content="We benchmarked 100+ models on our actual task and found a much cheaper alternative that works just as well." data-next-head=""/><meta property="og:title" content="Without Benchmarking LLMs, You&#x27;re Likely Overpaying 5-10x | Karl Lorey" data-next-head=""/><meta property="og:description" content="We benchmarked 100+ models on our actual task and found a much cheaper alternative that works just as well." data-next-head=""/><meta property="og:type" content="article" data-next-head=""/><meta property="og:url" content="https://karllorey.com/posts/without-benchmarking-llms-youre-overpaying" data-next-head=""/><meta property="og:site_name" content="Karl Lorey" data-next-head=""/><meta property="og:image" content="https://karllorey.com/img/og/without-benchmarking-llms-youre-overpaying.jpg" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta name="twitter:site" content="@karllorey" data-next-head=""/><meta name="twitter:title" content="Without Benchmarking LLMs, You&#x27;re Likely Overpaying 5-10x | Karl Lorey" data-next-head=""/><meta name="twitter:description" content="We benchmarked 100+ models on our actual task and found a much cheaper alternative that works just as well." data-next-head=""/><meta name="twitter:image" content="https://karllorey.com/img/og/without-benchmarking-llms-youre-overpaying.jpg" data-next-head=""/><meta property="article:published_time" content="2026-01-20T17:35:00.000Z" data-next-head=""/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/chunks/c086acbfa1de9d01.css" as="style"/><link rel="stylesheet" href="/_next/static/chunks/c086acbfa1de9d01.css" data-n-g=""/><noscript data-n-css=""></noscript><script src="/_next/static/chunks/707c9abbd82b80aa.js" defer=""></script><script src="/_next/static/chunks/afe01d89e7d3299e.js" defer=""></script><script src="/_next/static/chunks/ed86aa16915d4415.js" defer=""></script><script src="/_next/static/chunks/a9e06f39ec541142.js" defer=""></script><script src="/_next/static/chunks/turbopack-574ddab8a7ead8d6.js" defer=""></script><script src="/_next/static/chunks/2aa964a266236714.js" defer=""></script><script src="/_next/static/chunks/31953ed9a0e944a2.js" defer=""></script><script src="/_next/static/chunks/turbopack-baf907aa2fc58027.js" defer=""></script><script src="/_next/static/C8W0tfz9ArLWlQxgoxdq2/_ssgManifest.js" defer=""></script><script src="/_next/static/C8W0tfz9ArLWlQxgoxdq2/_buildManifest.js" defer=""></script></head><body class="antialiased"><link rel="preload" as="image" href="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/llm-as-judge.png"/><link rel="preload" as="image" href="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/pareto-frontier.png"/><link rel="preload" as="image" href="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-create-benchmark.png"/><link rel="preload" as="image" href="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-model-comparison.png"/><link rel="preload" as="image" href="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-results.png"/><div id="__next"><div class="max-w-xl mx-auto p-8"><header class="text-left md:text-center py-8 md:py-16"><div class="title py-2 md:py-4 ml-[-0.05em] md:ml-0"><a href="/">Karl Lorey</a></div><nav class="my-2 md:my-5"><ul class="flex flex-wrap justify-start md:justify-center gap-4"><li><a href="/human">Human</a></li><li><a href="/founder">Founder</a></li><li><a href="/portfolio">Builder</a></li><li><a href="/techie">Techie</a></li><li><a href="/vc">Investor</a></li><li><a href="/blog">Lorey Ipsum</a></li></ul></nav></header><hr class="mb-16"/><main class="content"><h1>Without Benchmarking LLMs, You&#x27;re Likely Overpaying 5-10x</h1><div class="text-gray-400 text-sm mb-8">Jan 20, 2026</div><div class="markdown"><p>Last month I helped a friend cut his LLM API bill by 80%.</p>
<p>He&#x27;s a non-technical founder building an AI-powered business.
Like most people, he picked GPT-5 because it&#x27;s the default:
You have the API already,
it has solid benchmarks,
everyone uses it,
why bother?!</p>
<p>But as usage grew, so did his bill.
$1,500/month for API calls alone.</p>
<p>So we benchmarked his actual prompts against 100+ models
and quickly realized that while GPT-5 is a solid choice,
it almost never is the cheapest and there are always cheaper options
with comparable quality.
Figuring out which saved him thousands of dollars in the process.
Here&#x27;s how we did it.</p>
<h2>The Problem: Benchmarks don&#x27;t predict performance on your task</h2>
<p>When picking an LLM, most people just choose a model from their favorite provider.
For me, that&#x27;s Anthropic, so depending on the task, I pick Opus, Sonnet, or Haiku.
If you&#x27;re sophisticated, you might check Artificial Analysis, or LM Arena,
or whatever benchmark seems relevant:
GPQA Diamond, AIME, SWE Bench, MATH 500, Humanity&#x27;s Last Exam, ARC-AGI, MMLU...</p>
<p>But let&#x27;s not fool ourselves here:
none of these predict performance on your specific task.
A model that tops reasoning benchmarks might be mediocre at damage cost estimation.
Or customer support in your customers&#x27; native language.
Or data extraction via Playwright.
Or whatever you&#x27;re actually building.</p>
<p>At best, they&#x27;re a rough indicator of performance.
And they do not account for costs at all.</p>
<p>The only way to know is to test on your actual prompts.
And make a decision considering quality, cost, and latency.</p>
<h2>Building benchmarks ourselves</h2>
<p>So to figure this out, we built our own benchmarks.
Let me walk through one use case: customer support.</p>
<h3>Step 1: Collect real examples</h3>
<p>We extracted actual support chats via <a href="https://whapi.cloud/">WHAPI</a>.
Each chat gave us the conversation history, the customer&#x27;s latest message, and the response my friend actually sent.
My friend also gave me the prompts he used manually and inside this chat tool to generate responses.
Based on this, we selected around 50 chats.
A lot with frequently asked questions, but also some edge cases where we wanted the LLM to behave in a certain way.</p>
<h3>Step 2: Define the expected output</h3>
<p>For each example, we used my friend&#x27;s actual response as the expected output.
We also defined some ranking criteria, for example:</p>
<blockquote>
<p>A good answer tells the customer that this product costs 5.99 and offers to take an order right now.</p>
</blockquote>
<p>Or:</p>
<blockquote>
<p>A good answer tells the customer that the return policy gives customers 30 days to send back the order, but that they sent their return over two months after receiving it.</p>
</blockquote>
<p>You get the idea.</p>
<h3>Step 3: Create the benchmark dataset</h3>
<p>We now had a simple dataset:
the prompt (conversation + instructions) and the expected response.</p>
<p>As you see, this is a generic format that could be used for all use cases.
For every prompt, you define the expected response.
If you know that a specific model works great, you can even use this to generate the response and refine if necessary.</p>
<h3>Step 4: Run all models</h3>
<p>We then ran this dataset across all the LLMs we wanted to benchmark.
To make implementation as easy as possible, we chose <a href="https://openrouter.ai/">OpenRouter</a> to get a broad set of LLMs behind the same API.
The beauty of OpenRouter is that you can use the standard OpenAI SDK and just swap out the model name:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI

client = OpenAI(
  base_url=<span class="hljs-string">&quot;https://openrouter.ai/api/v1&quot;</span>,
  api_key=<span class="hljs-string">&quot;&lt;OPENROUTER_API_KEY&gt;&quot;</span>,
)

completion = client.chat.completions.create(
  model=<span class="hljs-string">&quot;openai/gpt-5&quot;</span>,  <span class="hljs-comment"># or &quot;anthropic/claude-opus-4.5&quot;, &quot;google/gemini-3-pro-preview&quot;, ...</span>
  messages=[{<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Hello!&quot;</span>}]
)
</code></pre>
<p>This made it trivial to benchmark all models with the same code.
Running this across 50+ models gave us a dataframe with:
prompt, expected response, and actual response per model.</p>
<p>As you quickly realize, this is more data than you can evaluate manually.
So we needed a plan:
LLMs to the rescue, again.</p>
<h3>Step 5: Scoring with LLM-as-judge</h3>
<p>Since manually comparing hundreds of responses is not feasible,
we used an <a href="https://huggingface.co/learn/cookbook/en/llm_judge">LLM as a judge</a>.
For each sample, we used Opus 4.5 to score how well the actual response matched the expected response on a scale of 1-10.
This is why we set very specific criteria in step 2:
The LLMs were able to score much more reliably and consistently when given concrete scoring instructions.
We also spot-checked a sample of these scores against our own judgment to make sure the judge was reliable.
For example, sometimes imprecision in the expected answer led to the &quot;judge&quot; model applying scores wrong.
So this was more iterative than this list suggests.
That&#x27;s why we prompted for not only scores, but also the reasoning behind them.</p>
<p>We used the same approach for his other use cases.
Prompt, expected answer, and then one answer per model along with the judge model&#x27;s evaluation.</p>
<figure class="my-8"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/llm-as-judge.png" alt="Dataset creation" class="border border-gray-200 hover:border-gray-300 transition-colors cursor-zoom-in"/><figcaption class="mt-2 text-center text-sm text-gray-600">Dataset creation</figcaption><dialog class="fixed inset-0 m-0 h-screen w-screen max-h-none max-w-none bg-black/90 p-4 backdrop:bg-transparent"><div class="flex h-full w-full items-center justify-center"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/llm-as-judge.png" alt="Dataset creation" class="max-h-full max-w-full cursor-zoom-out object-contain"/></div></dialog></figure>
<h2>Deciding on the best model</h2>
<p>Now that we had a score to measure quality per LLM, the question was:
Which model should we choose?
In practice, you want a model that provides a balance of quality, cost, and latency.
For our customer support case, latency was important. We couldn&#x27;t wait for GPT-5 that took up to a minute with enough context, even though it provided great answers.
In contrast, for our other use case, damage cost estimation, we wanted the results to be as good as possible for a reasonable cost, however long they take.</p>
<p>This made us realize, we needed to measure both cost and latency, too.</p>
<ul>
<li>Cost: For cost, we quickly realized that simply comparing token costs is not enough:
Since response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,
we decided to measure overall costs per answer and thus the average costs per use case / benchmark.</li>
<li>Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.
Of course, that differs for chat applications where time to first token, etc. can be essential UX, too.</li>
</ul>
<p>This finally gave us a list of models per use case with quality, cost, and latency.
To decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.</p>
<p>In theory, there&#x27;s a concept called <a href="https://en.wikipedia.org/wiki/Pareto_front">Pareto Efficiency</a> that can be applied:
For a formal definition, the linked Wikipedia article does a better job than me.
For the informal definition, here&#x27;s my take:</p>
<blockquote>
<p>Given that you have a benchmark across 100 LLMs with a cost and score (let&#x27;s forget latency for a second).
There&#x27;s no point in comparing all 100 LLMs.
For most LLMs, you will find a model that is cheaper AND better.
This means there&#x27;s no point in looking at it, as there is another one that&#x27;s better in both dimensions.
Checking this for all LLMs in a benchmark,
you get a list of models that have no model that&#x27;s both cheaper AND better,
the Pareto Frontier:
The best LLMs for a given price.</p>
</blockquote>
<p>Here&#x27;s my attempt to visualize this:
I&#x27;ve plotted the price of a model on the x-axis
and the response quality on the y-axis.
The LLMs we benchmarked are the dots.
For all models plotted in blue
there is no model that&#x27;s cheaper and better.
Connecting these gives you the Pareto frontier:
the best models for a given price.</p>
<figure class="my-8"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/pareto-frontier.png" alt="pareto frontier" class="border border-gray-200 hover:border-gray-300 transition-colors cursor-zoom-in"/><figcaption class="mt-2 text-center text-sm text-gray-600">pareto frontier</figcaption><dialog class="fixed inset-0 m-0 h-screen w-screen max-h-none max-w-none bg-black/90 p-4 backdrop:bg-transparent"><div class="flex h-full w-full items-center justify-center"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/pareto-frontier.png" alt="pareto frontier" class="max-h-full max-w-full cursor-zoom-out object-contain"/></div></dialog></figure>
<p>Looking at it, it also becomes obvious that looking at other models makes no sense when optimizing for quality and cost.</p>
<h2>Saving $1000 monthly by switching the models</h2>
<p>With these benchmark results, we found models with comparable quality at up to 10x lower cost.
My friend chose a more conservative option that still cut costs by 5x, saving him over $1000/month.</p>
<p>This process was painful enough that I built a tool to automate it.</p>
<h2>evalry: a tool to benchmark your use case across 300+ LLMs</h2>
<p>Benchmarking and truly finding an optimal model is more complex than we initially thought.
That&#x27;s why my friend never did it, that&#x27;s why I usually don&#x27;t do it.
You need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.
Manually testing even 5 models can quickly become a multi-hour endeavor.
And new models drop weekly. Keeping up is impossible.
The same model in a month? Half the price because some <a href="https://siboehm.com">transformer wizard</a> dropped inference costs in half.</p>
<p>So to help my friend and anyone else with the same problem, I built <a href="https://evalry.com">Evalry</a>.
It provides all this in one simple tool that does the heavy lifting for you:
It tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.</p>
<figure class="my-8"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-create-benchmark.png" alt="Create a benchmark in Evalry" class="border border-gray-200 hover:border-gray-300 transition-colors cursor-zoom-in"/><figcaption class="mt-2 text-center text-sm text-gray-600">Create a benchmark in Evalry</figcaption><dialog class="fixed inset-0 m-0 h-screen w-screen max-h-none max-w-none bg-black/90 p-4 backdrop:bg-transparent"><div class="flex h-full w-full items-center justify-center"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-create-benchmark.png" alt="Create a benchmark in Evalry" class="max-h-full max-w-full cursor-zoom-out object-contain"/></div></dialog></figure>
<figure class="my-8"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-model-comparison.png" alt="Model comparison in Evalry" class="border border-gray-200 hover:border-gray-300 transition-colors cursor-zoom-in"/><figcaption class="mt-2 text-center text-sm text-gray-600">Model comparison in Evalry</figcaption><dialog class="fixed inset-0 m-0 h-screen w-screen max-h-none max-w-none bg-black/90 p-4 backdrop:bg-transparent"><div class="flex h-full w-full items-center justify-center"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-model-comparison.png" alt="Model comparison in Evalry" class="max-h-full max-w-full cursor-zoom-out object-contain"/></div></dialog></figure>
<figure class="my-8"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-results.png" alt="Benchmark results in Evalry" class="border border-gray-200 hover:border-gray-300 transition-colors cursor-zoom-in"/><figcaption class="mt-2 text-center text-sm text-gray-600">Benchmark results in Evalry</figcaption><dialog class="fixed inset-0 m-0 h-screen w-screen max-h-none max-w-none bg-black/90 p-4 backdrop:bg-transparent"><div class="flex h-full w-full items-center justify-center"><img src="/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-results.png" alt="Benchmark results in Evalry" class="max-h-full max-w-full cursor-zoom-out object-contain"/></div></dialog></figure>
<p>I&#x27;m also planning to set up continuous monitoring. So when a better model appears, that&#x27;s cheaper, faster, or higher quality for your use case, you get notified.</p>
<p>So if you&#x27;re paying for LLM APIs and have never tested alternatives on your actual prompts, you&#x27;re likely overpaying.
Give <a href="https://evalry.com">Evalry</a> a try. It takes 5 minutes to find out if there&#x27;s a better model for your use case.
Or if you&#x27;re short on time, <a href="https://evalry.com/models">find the model you&#x27;re currently using</a> and try the five models that have similar performance on average.</p></div></main><hr class="my-16"/><footer class="text-center text-sm py-10"><p>Karl Lorey: founder, builder, investor living in Germany.<br/>Reach me at mail (at) karllorey (dot) com or find me here:</p><p class="flex justify-center gap-4 flex-wrap text-lg mt-4"><a href="https://github.com/lorey" target="_blank" rel="noopener noreferrer" title="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://www.linkedin.com/in/karllorey" target="_blank" rel="noopener noreferrer" title="LinkedIn"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a href="https://x.com/karllorey" target="_blank" rel="noopener noreferrer" title="X"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a><a href="https://bsky.app/profile/karllorey.bsky.social" target="_blank" rel="noopener noreferrer" title="Bluesky"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z"></path></svg></a><a href="https://www.crunchbase.com/person/karl-lorey" target="_blank" rel="noopener noreferrer" title="Crunchbase"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M234.5 5.7c13.9-5 29.1-5 43.1 0l192 68.6C495 83.4 512 107.5 512 134.6l0 242.9c0 27-17 51.2-42.5 60.3l-192 68.6c-13.9 5-29.1 5-43.1 0l-192-68.6C17 428.6 0 404.5 0 377.4L0 134.6c0-27 17-51.2 42.5-60.3l192-68.6zM256 66L82.3 128 256 190l173.7-62L256 66zm32 368.6l160-57.1 0-188L288 246.6l0 188z"></path></svg></a><a href="https://www.producthunt.com/@karllorey" target="_blank" rel="noopener noreferrer" title="Product Hunt"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M326.3 218.8c0 20.5-16.7 37.2-37.2 37.2h-70.3v-74.4h70.3c20.5 0 37.2 16.7 37.2 37.2zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-128.1-37.2c0-47.9-38.9-86.8-86.8-86.8H169.2v248h49.6v-74.4h70.3c47.9 0 86.8-38.9 86.8-86.8z"></path></svg></a><a href="https://angel.co/karllorey" target="_blank" rel="noopener noreferrer" title="AngelList"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M347.1 215.4c11.7-32.6 45.4-126.9 45.4-157.1 0-26.6-15.7-48.9-43.7-48.9-44.6 0-84.6 131.7-97.1 163.1C242 144 196.6 0 156.6 0c-31.1 0-45.7 22.9-45.7 51.7 0 35.3 34.2 126.8 46.6 162-6.3-2.3-13.1-4.3-20-4.3-23.4 0-48.3 29.1-48.3 52.6 0 8.9 4.9 21.4 8 29.7-36.9 10-51.1 34.6-51.1 71.7C46 435.6 114.4 512 210.6 512c118 0 191.4-88.6 191.4-202.9 0-43.1-6.9-82-54.9-93.7zM311.7 108c4-12.3 21.1-64.3 37.1-64.3 8.6 0 10.9 8.9 10.9 16 0 19.1-38.6 124.6-47.1 148l-34-6 33.1-93.7zM142.3 48.3c0-11.9 14.5-45.7 46.3 47.1l34.6 100.3c-15.6-1.3-27.7-3-35.4 1.4-10.9-28.8-45.5-119.7-45.5-148.8zM140 244c29.3 0 67.1 94.6 67.1 107.4 0 5.1-4.9 11.4-10.6 11.4-20.9 0-76.9-76.9-76.9-97.7.1-7.7 12.7-21.1 20.4-21.1zm184.3 186.3c-29.1 32-66.3 48.6-109.7 48.6-59.4 0-106.3-32.6-128.9-88.3-17.1-43.4 3.8-68.3 20.6-68.3 11.4 0 54.3 60.3 54.3 73.1 0 4.9-7.7 8.3-11.7 8.3-16.1 0-22.4-15.5-51.1-51.4-29.7 29.7 20.5 86.9 58.3 86.9 26.1 0 43.1-24.2 38-42 3.7 0 8.3.3 11.7-.6 1.1 27.1 9.1 59.4 41.7 61.7 0-.9 2-7.1 2-7.4 0-17.4-10.6-32.6-10.6-50.3 0-28.3 21.7-55.7 43.7-71.7 8-6 17.7-9.7 27.1-13.1 9.7-3.7 20-8 27.4-15.4-1.1-11.2-5.7-21.1-16.9-21.1-27.7 0-120.6 4-120.6-39.7 0-6.7.1-13.1 17.4-13.1 32.3 0 114.3 8 138.3 29.1 18.1 16.1 24.3 113.2-31 174.7zm-98.6-126c9.7 3.1 19.7 4 29.7 6-7.4 5.4-14 12-20.3 19.1-2.8-8.5-6.2-16.8-9.4-25.1z"></path></svg></a><a href="https://gitlab.com/lorey" target="_blank" rel="noopener noreferrer" title="GitLab"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M503.5 204.6L502.8 202.8L433.1 21.02C431.7 17.45 429.2 14.43 425.9 12.38C423.5 10.83 420.8 9.865 417.9 9.57C415 9.275 412.2 9.653 409.5 10.68C406.8 11.7 404.4 13.34 402.4 15.46C400.5 17.58 399.1 20.13 398.3 22.9L351.3 166.9H160.8L113.7 22.9C112.9 20.13 111.5 17.59 109.6 15.47C107.6 13.35 105.2 11.72 102.5 10.7C99.86 9.675 96.98 9.295 94.12 9.587C91.26 9.878 88.51 10.83 86.08 12.38C82.84 14.43 80.33 17.45 78.92 21.02L9.267 202.8L8.543 204.6C-1.484 230.8-2.72 259.6 5.023 286.6C12.77 313.5 29.07 337.3 51.47 354.2L51.74 354.4L52.33 354.8L158.3 434.3L210.9 474L242.9 498.2C246.6 500.1 251.2 502.5 255.9 502.5C260.6 502.5 265.2 500.1 268.9 498.2L300.9 474L353.5 434.3L460.2 354.4L460.5 354.1C482.9 337.2 499.2 313.5 506.1 286.6C514.7 259.6 513.5 230.8 503.5 204.6z"></path></svg></a><a href="https://www.goodreads.com/karllorey" target="_blank" rel="noopener noreferrer" title="Goodreads"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M299.9 191.2c5.1 37.3-4.7 79-35.9 100.7-22.3 15.5-52.8 14.1-70.8 5.7-37.1-17.3-49.5-58.6-46.8-97.2 4.3-60.9 40.9-87.9 75.3-87.5 46.9-.2 71.8 31.8 78.2 78.3zM448 88v336c0 30.9-25.1 56-56 56H56c-30.9 0-56-25.1-56-56V88c0-30.9 25.1-56 56-56h336c30.9 0 56 25.1 56 56zM330 313.2s-.1-34-.1-217.3h-29v40.3c-.8.3-1.2-.5-1.6-1.2-9.6-20.7-35.9-46.3-76-46-51.9.4-87.2 31.2-100.6 77.8-4.3 14.9-5.8 30.1-5.5 45.6 1.7 77.9 45.1 117.8 112.4 115.2 28.9-1.1 54.5-17 69-45.2.5-1 1.1-1.9 1.7-2.9.2.1.4.1.6.2.3 3.8.2 30.7.1 34.5-.2 14.8-2 29.5-7.2 43.5-7.8 21-22.3 34.7-44.5 39.5-17.8 3.9-35.6 3.8-53.2-1.2-21.5-6.1-36.5-19-41.1-41.8-.3-1.6-1.3-1.3-2.3-1.3h-26.8c.8 10.6 3.2 20.3 8.5 29.2 24.2 40.5 82.7 48.5 128.2 37.4 49.9-12.3 67.3-54.9 67.4-106.3z"></path></svg></a><a href="https://www.instagram.com/karllorey" target="_blank" rel="noopener noreferrer" title="Instagram"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"></path></svg></a><a href="https://medium.com/@karllorey" target="_blank" rel="noopener noreferrer" title="Medium"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M180.5,74.262C80.813,74.262,0,155.633,0,256S80.819,437.738,180.5,437.738,361,356.373,361,256,280.191,74.262,180.5,74.262Zm288.25,10.646c-49.845,0-90.245,76.619-90.245,171.095s40.406,171.1,90.251,171.1,90.251-76.619,90.251-171.1H559C559,161.5,518.6,84.908,468.752,84.908Zm139.506,17.821c-17.526,0-31.735,68.628-31.735,153.274s14.2,153.274,31.735,153.274S640,340.631,640,256C640,171.351,625.785,102.729,608.258,102.729Z"></path></svg></a><a href="https://substack.com/@karllorey" target="_blank" rel="noopener noreferrer" title="Substack"><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M22.539 8.242H1.46V5.406h21.08v2.836zM1.46 10.812V24L12 18.11 22.54 24V10.812H1.46zM22.54 0H1.46v2.836h21.08V0z"></path></svg></a><a href="https://www.meetup.com/members/196097665/" target="_blank" rel="noopener noreferrer" title="Meetup"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M99 414.3c1.1 5.7-2.3 11.1-8 12.3-5.4 1.1-10.9-2.3-12-8-1.1-5.4 2.3-11.1 7.7-12.3 5.4-1.2 11.1 2.3 12.3 8zm143.1 71.4c-6.3 4.6-8 13.4-3.7 20 4.6 6.6 13.4 8.3 20 3.7 6.3-4.6 8-13.4 3.4-20-4.2-6.5-13.1-8.3-19.7-3.7zm-86-462.3c6.3-1.4 10.3-7.7 8.9-14-1.1-6.6-7.4-10.6-13.7-9.1-6.3 1.4-10.3 7.7-9.1 14 1.4 6.6 7.6 10.6 13.9 9.1zM34.4 226.3c-10-6.9-23.7-4.3-30.6 6-6.9 10-4.3 24 5.7 30.9 10 7.1 23.7 4.6 30.6-5.7 6.9-10.4 4.3-24.1-5.7-31.2zm272-170.9c10.6-6.3 13.7-20 7.7-30.3-6.3-10.6-19.7-14-30-7.7s-13.7 20-7.4 30.6c6 10.3 19.4 13.7 29.7 7.4zm-191.1 58c7.7-5.4 9.4-16 4.3-23.7s-15.7-9.4-23.1-4.3c-7.7 5.4-9.4 16-4.3 23.7 5.1 7.8 15.6 9.5 23.1 4.3zm372.3 156c-7.4 1.7-12.3 9.1-10.6 16.9 1.4 7.4 8.9 12.3 16.3 10.6 7.4-1.4 12.3-8.9 10.6-16.6-1.5-7.4-8.9-12.3-16.3-10.9zm39.7-56.8c-1.1-5.7-6.6-9.1-12-8-5.7 1.1-9.1 6.9-8 12.6 1.1 5.4 6.6 9.1 12.3 8 5.4-1.5 9.1-6.9 7.7-12.6zM447 138.9c-8.6 6-10.6 17.7-4.9 26.3 5.7 8.6 17.4 10.6 26 4.9 8.3-6 10.3-17.7 4.6-26.3-5.7-8.7-17.4-10.9-25.7-4.9zm-6.3 139.4c26.3 43.1 15.1 100-26.3 129.1-17.4 12.3-37.1 17.7-56.9 17.1-12 47.1-69.4 64.6-105.1 32.6-1.1.9-2.6 1.7-3.7 2.9-39.1 27.1-92.3 17.4-119.4-22.3-9.7-14.3-14.6-30.6-15.1-46.9-65.4-10.9-90-94-41.1-139.7-28.3-46.9.6-107.4 53.4-114.9C151.6 70 234.1 38.6 290.1 82c67.4-22.3 136.3 29.4 130.9 101.1 41.1 12.6 52.8 66.9 19.7 95.2zm-70 74.3c-3.1-20.6-40.9-4.6-43.1-27.1-3.1-32 43.7-101.1 40-128-3.4-24-19.4-29.1-33.4-29.4-13.4-.3-16.9 2-21.4 4.6-2.9 1.7-6.6 4.9-11.7-.3-6.3-6-11.1-11.7-19.4-12.9-12.3-2-17.7 2-26.6 9.7-3.4 2.9-12 12.9-20 9.1-3.4-1.7-15.4-7.7-24-11.4-16.3-7.1-40 4.6-48.6 20-12.9 22.9-38 113.1-41.7 125.1-8.6 26.6 10.9 48.6 36.9 47.1 11.1-.6 18.3-4.6 25.4-17.4 4-7.4 41.7-107.7 44.6-112.6 2-3.4 8.9-8 14.6-5.1 5.7 3.1 6.9 9.4 6 15.1-1.1 9.7-28 70.9-28.9 77.7-3.4 22.9 26.9 26.6 38.6 4 3.7-7.1 45.7-92.6 49.4-98.3 4.3-6.3 7.4-8.3 11.7-8 3.1 0 8.3.9 7.1 10.9-1.4 9.4-35.1 72.3-38.9 87.7-4.6 20.6 6.6 41.4 24.9 50.6 11.4 5.7 62.5 15.7 58.5-11.1zm5.7 92.3c-10.3 7.4-12.9 22-5.7 32.6 7.1 10.6 21.4 13.1 32 6 10.6-7.4 13.1-22 6-32.6-7.4-10.6-21.7-13.5-32.3-6z"></path></svg></a><a href="https://www.researchgate.net/profile/Karl_Lorey" target="_blank" rel="noopener noreferrer" title="ResearchGate"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="inline" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 32v448h448V32H0zm262.2 334.4c-6.6 3-33.2 6-50-14.2-9.2-10.6-25.3-33.3-42.2-63.6-8.9 0-14.7 0-21.4-.6v46.4c0 23.5 6 21.2 25.8 23.9v8.1c-6.9-.3-23.1-.8-35.6-.8-13.1 0-26.1.6-33.6.8v-8.1c15.5-2.9 22-1.3 22-23.9V225c0-22.6-6.4-21-22-23.9V193c25.8 1 53.1-.6 70.9-.6 31.7 0 55.9 14.4 55.9 45.6 0 21.1-16.7 42.2-39.2 47.5 13.6 24.2 30 45.6 42.2 58.9 7.2 7.8 17.2 14.7 27.2 14.7v7.3zm22.9-135c-23.3 0-32.2-15.7-32.2-32.2V167c0-12.2 8.8-30.4 34-30.4s30.4 17.9 30.4 17.9l-10.7 7.2s-5.5-12.5-19.7-12.5c-7.9 0-19.7 7.3-19.7 19.7v26.8c0 13.4 6.6 23.3 17.9 23.3 14.1 0 21.5-10.9 21.5-26.8h-17.9v-10.7h30.4c0 20.5 4.7 49.9-34 49.9zm-116.5 44.7c-9.4 0-13.6-.3-20-.8v-69.7c6.4-.6 15-.6 22.5-.6 23.3 0 37.2 12.2 37.2 34.5 0 21.9-15 36.6-39.7 36.6z"></path></svg></a></p><div class="mt-10 space-y-2"><p class="pb-0"><a href="/legal">Legal</a> · <a href="/privacy">Privacy</a> · <a href="https://github.com/lorey/karllorey.com">Source</a></p><p class="pb-0">© 2025 Karl Lorey.<!-- --> <!-- -->Updated<!-- --> <!-- -->Jan 20, 2026<!-- --> <!-- -->(<!-- -->7a3a350<!-- -->)</p></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Without Benchmarking LLMs, You're Likely Overpaying 5-10x","slug":"without-benchmarking-llms-youre-overpaying","date":"2026-01-20T17:35:00.000Z","tags":"LLM, AI, Tools, Startups","category":"Projects","description":"We benchmarked 100+ models on our actual task and found a much cheaper alternative that works just as well.","type":"text","status":"published","content":"\nLast month I helped a friend cut his LLM API bill by 80%.\n\nHe's a non-technical founder building an AI-powered business.\nLike most people, he picked GPT-5 because it's the default:\nYou have the API already,\nit has solid benchmarks,\neveryone uses it,\nwhy bother?!\n\nBut as usage grew, so did his bill.\n$1,500/month for API calls alone.\n\nSo we benchmarked his actual prompts against 100+ models\nand quickly realized that while GPT-5 is a solid choice,\nit almost never is the cheapest and there are always cheaper options\nwith comparable quality.\nFiguring out which saved him thousands of dollars in the process.\nHere's how we did it.\n\n## The Problem: Benchmarks don't predict performance on your task\n\nWhen picking an LLM, most people just choose a model from their favorite provider.\nFor me, that's Anthropic, so depending on the task, I pick Opus, Sonnet, or Haiku.\nIf you're sophisticated, you might check Artificial Analysis, or LM Arena,\nor whatever benchmark seems relevant:\nGPQA Diamond, AIME, SWE Bench, MATH 500, Humanity's Last Exam, ARC-AGI, MMLU...\n\nBut let's not fool ourselves here:\nnone of these predict performance on your specific task.\nA model that tops reasoning benchmarks might be mediocre at damage cost estimation.\nOr customer support in your customers' native language.\nOr data extraction via Playwright.\nOr whatever you're actually building.\n\nAt best, they're a rough indicator of performance.\nAnd they do not account for costs at all.\n\nThe only way to know is to test on your actual prompts.\nAnd make a decision considering quality, cost, and latency.\n\n## Building benchmarks ourselves\n\nSo to figure this out, we built our own benchmarks.\nLet me walk through one use case: customer support.\n\n### Step 1: Collect real examples\n\nWe extracted actual support chats via [WHAPI](https://whapi.cloud/).\nEach chat gave us the conversation history, the customer's latest message, and the response my friend actually sent.\nMy friend also gave me the prompts he used manually and inside this chat tool to generate responses.\nBased on this, we selected around 50 chats.\nA lot with frequently asked questions, but also some edge cases where we wanted the LLM to behave in a certain way.\n\n### Step 2: Define the expected output\n\nFor each example, we used my friend's actual response as the expected output.\nWe also defined some ranking criteria, for example:\n\n\u003e A good answer tells the customer that this product costs 5.99 and offers to take an order right now.\n\nOr:\n\n\u003e A good answer tells the customer that the return policy gives customers 30 days to send back the order, but that they sent their return over two months after receiving it.\n\nYou get the idea.\n\n### Step 3: Create the benchmark dataset\n\nWe now had a simple dataset:\nthe prompt (conversation + instructions) and the expected response.\n\nAs you see, this is a generic format that could be used for all use cases.\nFor every prompt, you define the expected response.\nIf you know that a specific model works great, you can even use this to generate the response and refine if necessary.\n\n### Step 4: Run all models\n\nWe then ran this dataset across all the LLMs we wanted to benchmark.\nTo make implementation as easy as possible, we chose [OpenRouter](https://openrouter.ai/) to get a broad set of LLMs behind the same API.\nThe beauty of OpenRouter is that you can use the standard OpenAI SDK and just swap out the model name:\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=\"\u003cOPENROUTER_API_KEY\u003e\",\n)\n\ncompletion = client.chat.completions.create(\n  model=\"openai/gpt-5\",  # or \"anthropic/claude-opus-4.5\", \"google/gemini-3-pro-preview\", ...\n  messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\nThis made it trivial to benchmark all models with the same code.\nRunning this across 50+ models gave us a dataframe with:\nprompt, expected response, and actual response per model.\n\nAs you quickly realize, this is more data than you can evaluate manually.\nSo we needed a plan:\nLLMs to the rescue, again.\n\n### Step 5: Scoring with LLM-as-judge\n\nSince manually comparing hundreds of responses is not feasible,\nwe used an [LLM as a judge](https://huggingface.co/learn/cookbook/en/llm_judge).\nFor each sample, we used Opus 4.5 to score how well the actual response matched the expected response on a scale of 1-10.\nThis is why we set very specific criteria in step 2:\nThe LLMs were able to score much more reliably and consistently when given concrete scoring instructions.\nWe also spot-checked a sample of these scores against our own judgment to make sure the judge was reliable.\nFor example, sometimes imprecision in the expected answer led to the \"judge\" model applying scores wrong.\nSo this was more iterative than this list suggests.\nThat's why we prompted for not only scores, but also the reasoning behind them.\n\nWe used the same approach for his other use cases.\nPrompt, expected answer, and then one answer per model along with the judge model's evaluation.\n\n![Dataset creation](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/llm-as-judge.png)\n\n## Deciding on the best model\n\nNow that we had a score to measure quality per LLM, the question was:\nWhich model should we choose?\nIn practice, you want a model that provides a balance of quality, cost, and latency.\nFor our customer support case, latency was important. We couldn't wait for GPT-5 that took up to a minute with enough context, even though it provided great answers.\nIn contrast, for our other use case, damage cost estimation, we wanted the results to be as good as possible for a reasonable cost, however long they take.\n\nThis made us realize, we needed to measure both cost and latency, too.\n\n- Cost: For cost, we quickly realized that simply comparing token costs is not enough:\n  Since response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,\n  we decided to measure overall costs per answer and thus the average costs per use case / benchmark.\n- Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.\n  Of course, that differs for chat applications where time to first token, etc. can be essential UX, too.\n\nThis finally gave us a list of models per use case with quality, cost, and latency.\nTo decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.\n\nIn theory, there's a concept called [Pareto Efficiency](https://en.wikipedia.org/wiki/Pareto_front) that can be applied:\nFor a formal definition, the linked Wikipedia article does a better job than me.\nFor the informal definition, here's my take:\n\n\u003e Given that you have a benchmark across 100 LLMs with a cost and score (let's forget latency for a second).\n\u003e There's no point in comparing all 100 LLMs.\n\u003e For most LLMs, you will find a model that is cheaper AND better.\n\u003e This means there's no point in looking at it, as there is another one that's better in both dimensions.\n\u003e Checking this for all LLMs in a benchmark,\n\u003e you get a list of models that have no model that's both cheaper AND better,\n\u003e the Pareto Frontier:\n\u003e The best LLMs for a given price.\n\nHere's my attempt to visualize this:\nI've plotted the price of a model on the x-axis\nand the response quality on the y-axis.\nThe LLMs we benchmarked are the dots.\nFor all models plotted in blue\nthere is no model that's cheaper and better.\nConnecting these gives you the Pareto frontier:\nthe best models for a given price.\n\n![pareto frontier](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/pareto-frontier.png)\n\nLooking at it, it also becomes obvious that looking at other models makes no sense when optimizing for quality and cost.\n\n## Saving $1000 monthly by switching the models\n\nWith these benchmark results, we found models with comparable quality at up to 10x lower cost.\nMy friend chose a more conservative option that still cut costs by 5x, saving him over $1000/month.\n\nThis process was painful enough that I built a tool to automate it.\n\n## evalry: a tool to benchmark your use case across 300+ LLMs\n\nBenchmarking and truly finding an optimal model is more complex than we initially thought.\nThat's why my friend never did it, that's why I usually don't do it.\nYou need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.\nManually testing even 5 models can quickly become a multi-hour endeavor.\nAnd new models drop weekly. Keeping up is impossible.\nThe same model in a month? Half the price because some [transformer wizard](https://siboehm.com) dropped inference costs in half.\n\nSo to help my friend and anyone else with the same problem, I built [Evalry](https://evalry.com).\nIt provides all this in one simple tool that does the heavy lifting for you:\nIt tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.\n\n![Create a benchmark in Evalry](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-create-benchmark.png)\n\n![Model comparison in Evalry](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-model-comparison.png)\n\n![Benchmark results in Evalry](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-results.png)\n\nI'm also planning to set up continuous monitoring. So when a better model appears, that's cheaper, faster, or higher quality for your use case, you get notified.\n\nSo if you're paying for LLM APIs and have never tested alternatives on your actual prompts, you're likely overpaying.\nGive [Evalry](https://evalry.com) a try. It takes 5 minutes to find out if there's a better model for your use case.\nOr if you're short on time, [find the model you're currently using](https://evalry.com/models) and try the five models that have similar performance on average.\n","folderName":"2026-without-benchmarking-llms-youre-likely-overpaying"},"mdxSource":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    h2: \"h2\",\n    h3: \"h3\",\n    img: \"img\",\n    li: \"li\",\n    p: \"p\",\n    pre: \"pre\",\n    span: \"span\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Last month I helped a friend cut his LLM API bill by 80%.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"He's a non-technical founder building an AI-powered business.\\nLike most people, he picked GPT-5 because it's the default:\\nYou have the API already,\\nit has solid benchmarks,\\neveryone uses it,\\nwhy bother?!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But as usage grew, so did his bill.\\n$1,500/month for API calls alone.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"So we benchmarked his actual prompts against 100+ models\\nand quickly realized that while GPT-5 is a solid choice,\\nit almost never is the cheapest and there are always cheaper options\\nwith comparable quality.\\nFiguring out which saved him thousands of dollars in the process.\\nHere's how we did it.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The Problem: Benchmarks don't predict performance on your task\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When picking an LLM, most people just choose a model from their favorite provider.\\nFor me, that's Anthropic, so depending on the task, I pick Opus, Sonnet, or Haiku.\\nIf you're sophisticated, you might check Artificial Analysis, or LM Arena,\\nor whatever benchmark seems relevant:\\nGPQA Diamond, AIME, SWE Bench, MATH 500, Humanity's Last Exam, ARC-AGI, MMLU...\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But let's not fool ourselves here:\\nnone of these predict performance on your specific task.\\nA model that tops reasoning benchmarks might be mediocre at damage cost estimation.\\nOr customer support in your customers' native language.\\nOr data extraction via Playwright.\\nOr whatever you're actually building.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"At best, they're a rough indicator of performance.\\nAnd they do not account for costs at all.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The only way to know is to test on your actual prompts.\\nAnd make a decision considering quality, cost, and latency.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Building benchmarks ourselves\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"So to figure this out, we built our own benchmarks.\\nLet me walk through one use case: customer support.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 1: Collect real examples\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"We extracted actual support chats via \", _jsx(_components.a, {\n        href: \"https://whapi.cloud/\",\n        children: \"WHAPI\"\n      }), \".\\nEach chat gave us the conversation history, the customer's latest message, and the response my friend actually sent.\\nMy friend also gave me the prompts he used manually and inside this chat tool to generate responses.\\nBased on this, we selected around 50 chats.\\nA lot with frequently asked questions, but also some edge cases where we wanted the LLM to behave in a certain way.\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 2: Define the expected output\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For each example, we used my friend's actual response as the expected output.\\nWe also defined some ranking criteria, for example:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"A good answer tells the customer that this product costs 5.99 and offers to take an order right now.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Or:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"A good answer tells the customer that the return policy gives customers 30 days to send back the order, but that they sent their return over two months after receiving it.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"You get the idea.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 3: Create the benchmark dataset\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We now had a simple dataset:\\nthe prompt (conversation + instructions) and the expected response.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"As you see, this is a generic format that could be used for all use cases.\\nFor every prompt, you define the expected response.\\nIf you know that a specific model works great, you can even use this to generate the response and refine if necessary.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 4: Run all models\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"We then ran this dataset across all the LLMs we wanted to benchmark.\\nTo make implementation as easy as possible, we chose \", _jsx(_components.a, {\n        href: \"https://openrouter.ai/\",\n        children: \"OpenRouter\"\n      }), \" to get a broad set of LLMs behind the same API.\\nThe beauty of OpenRouter is that you can use the standard OpenAI SDK and just swap out the model name:\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" openai \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" OpenAI\\n\\nclient = OpenAI(\\n  base_url=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"https://openrouter.ai/api/v1\\\"\"\n        }), \",\\n  api_key=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"\u003cOPENROUTER_API_KEY\u003e\\\"\"\n        }), \",\\n)\\n\\ncompletion = client.chat.completions.create(\\n  model=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"openai/gpt-5\\\"\"\n        }), \",  \", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# or \\\"anthropic/claude-opus-4.5\\\", \\\"google/gemini-3-pro-preview\\\", ...\"\n        }), \"\\n  messages=[{\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"role\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"user\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"content\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Hello!\\\"\"\n        }), \"}]\\n)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This made it trivial to benchmark all models with the same code.\\nRunning this across 50+ models gave us a dataframe with:\\nprompt, expected response, and actual response per model.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"As you quickly realize, this is more data than you can evaluate manually.\\nSo we needed a plan:\\nLLMs to the rescue, again.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 5: Scoring with LLM-as-judge\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Since manually comparing hundreds of responses is not feasible,\\nwe used an \", _jsx(_components.a, {\n        href: \"https://huggingface.co/learn/cookbook/en/llm_judge\",\n        children: \"LLM as a judge\"\n      }), \".\\nFor each sample, we used Opus 4.5 to score how well the actual response matched the expected response on a scale of 1-10.\\nThis is why we set very specific criteria in step 2:\\nThe LLMs were able to score much more reliably and consistently when given concrete scoring instructions.\\nWe also spot-checked a sample of these scores against our own judgment to make sure the judge was reliable.\\nFor example, sometimes imprecision in the expected answer led to the \\\"judge\\\" model applying scores wrong.\\nSo this was more iterative than this list suggests.\\nThat's why we prompted for not only scores, but also the reasoning behind them.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We used the same approach for his other use cases.\\nPrompt, expected answer, and then one answer per model along with the judge model's evaluation.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/llm-as-judge.png\",\n      alt: \"Dataset creation\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Deciding on the best model\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now that we had a score to measure quality per LLM, the question was:\\nWhich model should we choose?\\nIn practice, you want a model that provides a balance of quality, cost, and latency.\\nFor our customer support case, latency was important. We couldn't wait for GPT-5 that took up to a minute with enough context, even though it provided great answers.\\nIn contrast, for our other use case, damage cost estimation, we wanted the results to be as good as possible for a reasonable cost, however long they take.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This made us realize, we needed to measure both cost and latency, too.\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Cost: For cost, we quickly realized that simply comparing token costs is not enough:\\nSince response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,\\nwe decided to measure overall costs per answer and thus the average costs per use case / benchmark.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.\\nOf course, that differs for chat applications where time to first token, etc. can be essential UX, too.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This finally gave us a list of models per use case with quality, cost, and latency.\\nTo decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"In theory, there's a concept called \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Pareto_front\",\n        children: \"Pareto Efficiency\"\n      }), \" that can be applied:\\nFor a formal definition, the linked Wikipedia article does a better job than me.\\nFor the informal definition, here's my take:\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"Given that you have a benchmark across 100 LLMs with a cost and score (let's forget latency for a second).\\nThere's no point in comparing all 100 LLMs.\\nFor most LLMs, you will find a model that is cheaper AND better.\\nThis means there's no point in looking at it, as there is another one that's better in both dimensions.\\nChecking this for all LLMs in a benchmark,\\nyou get a list of models that have no model that's both cheaper AND better,\\nthe Pareto Frontier:\\nThe best LLMs for a given price.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Here's my attempt to visualize this:\\nI've plotted the price of a model on the x-axis\\nand the response quality on the y-axis.\\nThe LLMs we benchmarked are the dots.\\nFor all models plotted in blue\\nthere is no model that's cheaper and better.\\nConnecting these gives you the Pareto frontier:\\nthe best models for a given price.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/pareto-frontier.png\",\n      alt: \"pareto frontier\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Looking at it, it also becomes obvious that looking at other models makes no sense when optimizing for quality and cost.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Saving $1000 monthly by switching the models\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With these benchmark results, we found models with comparable quality at up to 10x lower cost.\\nMy friend chose a more conservative option that still cut costs by 5x, saving him over $1000/month.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This process was painful enough that I built a tool to automate it.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"evalry: a tool to benchmark your use case across 300+ LLMs\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Benchmarking and truly finding an optimal model is more complex than we initially thought.\\nThat's why my friend never did it, that's why I usually don't do it.\\nYou need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.\\nManually testing even 5 models can quickly become a multi-hour endeavor.\\nAnd new models drop weekly. Keeping up is impossible.\\nThe same model in a month? Half the price because some \", _jsx(_components.a, {\n        href: \"https://siboehm.com\",\n        children: \"transformer wizard\"\n      }), \" dropped inference costs in half.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So to help my friend and anyone else with the same problem, I built \", _jsx(_components.a, {\n        href: \"https://evalry.com\",\n        children: \"Evalry\"\n      }), \".\\nIt provides all this in one simple tool that does the heavy lifting for you:\\nIt tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.\"]\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-create-benchmark.png\",\n      alt: \"Create a benchmark in Evalry\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-model-comparison.png\",\n      alt: \"Model comparison in Evalry\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-results.png\",\n      alt: \"Benchmark results in Evalry\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I'm also planning to set up continuous monitoring. So when a better model appears, that's cheaper, faster, or higher quality for your use case, you get notified.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So if you're paying for LLM APIs and have never tested alternatives on your actual prompts, you're likely overpaying.\\nGive \", _jsx(_components.a, {\n        href: \"https://evalry.com\",\n        children: \"Evalry\"\n      }), \" a try. It takes 5 minutes to find out if there's a better model for your use case.\\nOr if you're short on time, \", _jsx(_components.a, {\n        href: \"https://evalry.com/models\",\n        children: \"find the model you're currently using\"\n      }), \" and try the five models that have similar performance on average.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"without-benchmarking-llms-youre-overpaying"},"buildId":"C8W0tfz9ArLWlQxgoxdq2","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>