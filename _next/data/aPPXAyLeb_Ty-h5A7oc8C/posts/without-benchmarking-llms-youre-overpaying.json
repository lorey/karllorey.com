{"pageProps":{"post":{"title":"Without Benchmarking LLMs, You're Likely Overpaying 5-10x","slug":"without-benchmarking-llms-youre-overpaying","date":"2026-01-20T17:35:00.000Z","tags":"LLM, AI, Tools, Startups","category":"Projects","description":"We benchmarked 100+ models on our actual task and found a much cheaper alternative that works just as well.","type":"text","status":"published","content":"\nLast month I helped a friend cut his LLM API bill by 80%.\n\nHe's a non-technical founder building an AI-powered business.\nLike most people, he picked GPT-5 because it's the default:\nYou have the API already,\nit has solid benchmarks,\neveryone uses it,\nwhy bother?!\n\nBut as usage grew, so did his bill.\n$1,500/month for API calls alone.\n\nSo we benchmarked his actual prompts against 100+ models\nand quickly realized that while GPT-5 is a solid choice,\nit almost never is the cheapest and there are always cheaper options\nwith comparable quality.\nFiguring out which saved him thousands of dollars in the process.\nHere's how we did it.\n\n## The Problem: Benchmarks don't predict performance on your task\n\nWhen picking an LLM, most people just choose a model from their favorite provider.\nFor me, that's Anthropic, so depending on the task, I pick Opus, Sonnet, or Haiku.\nIf you're sophisticated, you might check Artificial Analysis, or LM Arena,\nor whatever benchmark seems relevant:\nGPQA Diamond, AIME, SWE Bench, MATH 500, Humanity's Last Exam, ARC-AGI, MMLU...\n\nBut let's not fool ourselves here:\nnone of these predict performance on your specific task.\nA model that tops reasoning benchmarks might be mediocre at damage cost estimation.\nOr customer support in your customers' native language.\nOr data extraction via Playwright.\nOr whatever you're actually building.\n\nAt best, they're a rough indicator of performance.\nAnd they do not account for costs at all.\n\nThe only way to know is to test on your actual prompts.\nAnd make a decision considering quality, cost, and latency.\n\n## Building benchmarks ourselves\n\nSo to figure this out, we built our own benchmarks.\nLet me walk through one use case: customer support.\n\n### Step 1: Collect real examples\n\nWe extracted actual support chats via [WHAPI](https://whapi.cloud/).\nEach chat gave us the conversation history, the customer's latest message, and the response my friend actually sent.\nMy friend also gave me the prompts he used manually and inside this chat tool to generate responses.\nBased on this, we selected around 50 chats.\nA lot with frequently asked questions, but also some edge cases where we wanted the LLM to behave in a certain way.\n\n### Step 2: Define the expected output\n\nFor each example, we used my friend's actual response as the expected output.\nWe also defined some ranking criteria, for example:\n\n> A good answer tells the customer that this product costs 5.99 and offers to take an order right now.\n\nOr:\n\n> A good answer tells the customer that the return policy gives customers 30 days to send back the order, but that they sent their return over two months after receiving it.\n\nYou get the idea.\n\n### Step 3: Create the benchmark dataset\n\nWe now had a simple dataset:\nthe prompt (conversation + instructions) and the expected response.\n\nAs you see, this is a generic format that could be used for all use cases.\nFor every prompt, you define the expected response.\nIf you know that a specific model works great, you can even use this to generate the response and refine if necessary.\n\n### Step 4: Run all models\n\nWe then ran this dataset across all the LLMs we wanted to benchmark.\nTo make implementation as easy as possible, we chose [OpenRouter](https://openrouter.ai/) to get a broad set of LLMs behind the same API.\nThe beauty of OpenRouter is that you can use the standard OpenAI SDK and just swap out the model name:\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=\"<OPENROUTER_API_KEY>\",\n)\n\ncompletion = client.chat.completions.create(\n  model=\"openai/gpt-5\",  # or \"anthropic/claude-opus-4.5\", \"google/gemini-3-pro-preview\", ...\n  messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\nThis made it trivial to benchmark all models with the same code.\nRunning this across 50+ models gave us a dataframe with:\nprompt, expected response, and actual response per model.\n\nAs you quickly realize, this is more data than you can evaluate manually.\nSo we needed a plan:\nLLMs to the rescue, again.\n\n### Step 5: Scoring with LLM-as-judge\n\nSince manually comparing hundreds of responses is not feasible,\nwe used an [LLM as a judge](https://huggingface.co/learn/cookbook/en/llm_judge).\nFor each sample, we used Opus 4.5 to score how well the actual response matched the expected response on a scale of 1-10.\nThis is why we set very specific criteria in step 2:\nThe LLMs were able to score much more reliably and consistently when given concrete scoring instructions.\nWe also spot-checked a sample of these scores against our own judgment to make sure the judge was reliable.\nFor example, sometimes imprecision in the expected answer led to the \"judge\" model applying scores wrong.\nSo this was more iterative than this list suggests.\nThat's why we prompted for not only scores, but also the reasoning behind them.\n\nWe used the same approach for his other use cases.\nPrompt, expected answer, and then one answer per model along with the judge model's evaluation.\n\n![Dataset creation](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/llm-as-judge.png)\n\n## Deciding on the best model\n\nNow that we had a score to measure quality per LLM, the question was:\nWhich model should we choose?\nIn practice, you want a model that provides a balance of quality, cost, and latency.\nFor our customer support case, latency was important. We couldn't wait for GPT-5 that took up to a minute with enough context, even though it provided great answers.\nIn contrast, for our other use case, damage cost estimation, we wanted the results to be as good as possible for a reasonable cost, however long they take.\n\nThis made us realize, we needed to measure both cost and latency, too.\n\n- Cost: For cost, we quickly realized that simply comparing token costs is not enough:\n  Since response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,\n  we decided to measure overall costs per answer and thus the average costs per use case / benchmark.\n- Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.\n  Of course, that differs for chat applications where time to first token, etc. can be essential UX, too.\n\nThis finally gave us a list of models per use case with quality, cost, and latency.\nTo decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.\n\nIn theory, there's a concept called [Pareto Efficiency](https://en.wikipedia.org/wiki/Pareto_front) that can be applied:\nFor a formal definition, the linked Wikipedia article does a better job than me.\nFor the informal definition, here's my take:\n\n> Given that you have a benchmark across 100 LLMs with a cost and score (let's forget latency for a second).\n> There's no point in comparing all 100 LLMs.\n> For most LLMs, you will find a model that is cheaper AND better.\n> This means there's no point in looking at it, as there is another one that's better in both dimensions.\n> Checking this for all LLMs in a benchmark,\n> you get a list of models that have no model that's both cheaper AND better,\n> the Pareto Frontier:\n> The best LLMs for a given price.\n\nHere's my attempt to visualize this:\nI've plotted the price of a model on the x-axis\nand the response quality on the y-axis.\nThe LLMs we benchmarked are the dots.\nFor all models plotted in blue\nthere is no model that's cheaper and better.\nConnecting these gives you the Pareto frontier:\nthe best models for a given price.\n\n![pareto frontier](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/pareto-frontier.png)\n\nLooking at it, it also becomes obvious that looking at other models makes no sense when optimizing for quality and cost.\n\n## Saving $1000 monthly by switching the models\n\nWith these benchmark results, we found models with comparable quality at up to 10x lower cost.\nMy friend chose a more conservative option that still cut costs by 5x, saving him over $1000/month.\n\nThis process was painful enough that I built a tool to automate it.\n\n## evalry: a tool to benchmark your use case across 300+ LLMs\n\nBenchmarking and truly finding an optimal model is more complex than we initially thought.\nThat's why my friend never did it, that's why I usually don't do it.\nYou need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.\nManually testing even 5 models can quickly become a multi-hour endeavor.\nAnd new models drop weekly. Keeping up is impossible.\nThe same model in a month? Half the price because some [transformer wizard](https://siboehm.com) dropped inference costs in half.\n\nSo to help my friend and anyone else with the same problem, I built [Evalry](https://evalry.com).\nIt provides all this in one simple tool that does the heavy lifting for you:\nIt tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.\n\n![Create a benchmark in Evalry](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-create-benchmark.png)\n\n![Model comparison in Evalry](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-model-comparison.png)\n\n![Benchmark results in Evalry](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-results.png)\n\nI'm also planning to set up continuous monitoring. So when a better model appears, that's cheaper, faster, or higher quality for your use case, you get notified.\n\nSo if you're paying for LLM APIs and have never tested alternatives on your actual prompts, you're likely overpaying.\nGive [Evalry](https://evalry.com) a try. It takes 5 minutes to find out if there's a better model for your use case.\nOr if you're short on time, [find the model you're currently using](https://evalry.com/models) and try the five models that have similar performance on average.\n","folderName":"2026-without-benchmarking-llms-youre-likely-overpaying"},"mdxSource":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    code: \"code\",\n    h2: \"h2\",\n    h3: \"h3\",\n    img: \"img\",\n    li: \"li\",\n    p: \"p\",\n    pre: \"pre\",\n    span: \"span\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"Last month I helped a friend cut his LLM API bill by 80%.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"He's a non-technical founder building an AI-powered business.\\nLike most people, he picked GPT-5 because it's the default:\\nYou have the API already,\\nit has solid benchmarks,\\neveryone uses it,\\nwhy bother?!\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But as usage grew, so did his bill.\\n$1,500/month for API calls alone.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"So we benchmarked his actual prompts against 100+ models\\nand quickly realized that while GPT-5 is a solid choice,\\nit almost never is the cheapest and there are always cheaper options\\nwith comparable quality.\\nFiguring out which saved him thousands of dollars in the process.\\nHere's how we did it.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The Problem: Benchmarks don't predict performance on your task\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When picking an LLM, most people just choose a model from their favorite provider.\\nFor me, that's Anthropic, so depending on the task, I pick Opus, Sonnet, or Haiku.\\nIf you're sophisticated, you might check Artificial Analysis, or LM Arena,\\nor whatever benchmark seems relevant:\\nGPQA Diamond, AIME, SWE Bench, MATH 500, Humanity's Last Exam, ARC-AGI, MMLU...\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But let's not fool ourselves here:\\nnone of these predict performance on your specific task.\\nA model that tops reasoning benchmarks might be mediocre at damage cost estimation.\\nOr customer support in your customers' native language.\\nOr data extraction via Playwright.\\nOr whatever you're actually building.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"At best, they're a rough indicator of performance.\\nAnd they do not account for costs at all.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The only way to know is to test on your actual prompts.\\nAnd make a decision considering quality, cost, and latency.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Building benchmarks ourselves\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"So to figure this out, we built our own benchmarks.\\nLet me walk through one use case: customer support.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 1: Collect real examples\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"We extracted actual support chats via \", _jsx(_components.a, {\n        href: \"https://whapi.cloud/\",\n        children: \"WHAPI\"\n      }), \".\\nEach chat gave us the conversation history, the customer's latest message, and the response my friend actually sent.\\nMy friend also gave me the prompts he used manually and inside this chat tool to generate responses.\\nBased on this, we selected around 50 chats.\\nA lot with frequently asked questions, but also some edge cases where we wanted the LLM to behave in a certain way.\"]\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 2: Define the expected output\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"For each example, we used my friend's actual response as the expected output.\\nWe also defined some ranking criteria, for example:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"A good answer tells the customer that this product costs 5.99 and offers to take an order right now.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Or:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"A good answer tells the customer that the return policy gives customers 30 days to send back the order, but that they sent their return over two months after receiving it.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"You get the idea.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 3: Create the benchmark dataset\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We now had a simple dataset:\\nthe prompt (conversation + instructions) and the expected response.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"As you see, this is a generic format that could be used for all use cases.\\nFor every prompt, you define the expected response.\\nIf you know that a specific model works great, you can even use this to generate the response and refine if necessary.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 4: Run all models\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"We then ran this dataset across all the LLMs we wanted to benchmark.\\nTo make implementation as easy as possible, we chose \", _jsx(_components.a, {\n        href: \"https://openrouter.ai/\",\n        children: \"OpenRouter\"\n      }), \" to get a broad set of LLMs behind the same API.\\nThe beauty of OpenRouter is that you can use the standard OpenAI SDK and just swap out the model name:\"]\n    }), \"\\n\", _jsx(_components.pre, {\n      children: _jsxs(_components.code, {\n        className: \"hljs language-python\",\n        children: [_jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"from\"\n        }), \" openai \", _jsx(_components.span, {\n          className: \"hljs-keyword\",\n          children: \"import\"\n        }), \" OpenAI\\n\\nclient = OpenAI(\\n  base_url=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"https://openrouter.ai/api/v1\\\"\"\n        }), \",\\n  api_key=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"<OPENROUTER_API_KEY>\\\"\"\n        }), \",\\n)\\n\\ncompletion = client.chat.completions.create(\\n  model=\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"openai/gpt-5\\\"\"\n        }), \",  \", _jsx(_components.span, {\n          className: \"hljs-comment\",\n          children: \"# or \\\"anthropic/claude-opus-4.5\\\", \\\"google/gemini-3-pro-preview\\\", ...\"\n        }), \"\\n  messages=[{\", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"role\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"user\\\"\"\n        }), \", \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"content\\\"\"\n        }), \": \", _jsx(_components.span, {\n          className: \"hljs-string\",\n          children: \"\\\"Hello!\\\"\"\n        }), \"}]\\n)\\n\"]\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This made it trivial to benchmark all models with the same code.\\nRunning this across 50+ models gave us a dataframe with:\\nprompt, expected response, and actual response per model.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"As you quickly realize, this is more data than you can evaluate manually.\\nSo we needed a plan:\\nLLMs to the rescue, again.\"\n    }), \"\\n\", _jsx(_components.h3, {\n      children: \"Step 5: Scoring with LLM-as-judge\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Since manually comparing hundreds of responses is not feasible,\\nwe used an \", _jsx(_components.a, {\n        href: \"https://huggingface.co/learn/cookbook/en/llm_judge\",\n        children: \"LLM as a judge\"\n      }), \".\\nFor each sample, we used Opus 4.5 to score how well the actual response matched the expected response on a scale of 1-10.\\nThis is why we set very specific criteria in step 2:\\nThe LLMs were able to score much more reliably and consistently when given concrete scoring instructions.\\nWe also spot-checked a sample of these scores against our own judgment to make sure the judge was reliable.\\nFor example, sometimes imprecision in the expected answer led to the \\\"judge\\\" model applying scores wrong.\\nSo this was more iterative than this list suggests.\\nThat's why we prompted for not only scores, but also the reasoning behind them.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"We used the same approach for his other use cases.\\nPrompt, expected answer, and then one answer per model along with the judge model's evaluation.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/llm-as-judge.png\",\n      alt: \"Dataset creation\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Deciding on the best model\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now that we had a score to measure quality per LLM, the question was:\\nWhich model should we choose?\\nIn practice, you want a model that provides a balance of quality, cost, and latency.\\nFor our customer support case, latency was important. We couldn't wait for GPT-5 that took up to a minute with enough context, even though it provided great answers.\\nIn contrast, for our other use case, damage cost estimation, we wanted the results to be as good as possible for a reasonable cost, however long they take.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This made us realize, we needed to measure both cost and latency, too.\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Cost: For cost, we quickly realized that simply comparing token costs is not enough:\\nSince response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,\\nwe decided to measure overall costs per answer and thus the average costs per use case / benchmark.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.\\nOf course, that differs for chat applications where time to first token, etc. can be essential UX, too.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This finally gave us a list of models per use case with quality, cost, and latency.\\nTo decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"In theory, there's a concept called \", _jsx(_components.a, {\n        href: \"https://en.wikipedia.org/wiki/Pareto_front\",\n        children: \"Pareto Efficiency\"\n      }), \" that can be applied:\\nFor a formal definition, the linked Wikipedia article does a better job than me.\\nFor the informal definition, here's my take:\"]\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"Given that you have a benchmark across 100 LLMs with a cost and score (let's forget latency for a second).\\nThere's no point in comparing all 100 LLMs.\\nFor most LLMs, you will find a model that is cheaper AND better.\\nThis means there's no point in looking at it, as there is another one that's better in both dimensions.\\nChecking this for all LLMs in a benchmark,\\nyou get a list of models that have no model that's both cheaper AND better,\\nthe Pareto Frontier:\\nThe best LLMs for a given price.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Here's my attempt to visualize this:\\nI've plotted the price of a model on the x-axis\\nand the response quality on the y-axis.\\nThe LLMs we benchmarked are the dots.\\nFor all models plotted in blue\\nthere is no model that's cheaper and better.\\nConnecting these gives you the Pareto frontier:\\nthe best models for a given price.\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/pareto-frontier.png\",\n      alt: \"pareto frontier\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Looking at it, it also becomes obvious that looking at other models makes no sense when optimizing for quality and cost.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Saving $1000 monthly by switching the models\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With these benchmark results, we found models with comparable quality at up to 10x lower cost.\\nMy friend chose a more conservative option that still cut costs by 5x, saving him over $1000/month.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This process was painful enough that I built a tool to automate it.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"evalry: a tool to benchmark your use case across 300+ LLMs\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"Benchmarking and truly finding an optimal model is more complex than we initially thought.\\nThat's why my friend never did it, that's why I usually don't do it.\\nYou need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.\\nManually testing even 5 models can quickly become a multi-hour endeavor.\\nAnd new models drop weekly. Keeping up is impossible.\\nThe same model in a month? Half the price because some \", _jsx(_components.a, {\n        href: \"https://siboehm.com\",\n        children: \"transformer wizard\"\n      }), \" dropped inference costs in half.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So to help my friend and anyone else with the same problem, I built \", _jsx(_components.a, {\n        href: \"https://evalry.com\",\n        children: \"Evalry\"\n      }), \".\\nIt provides all this in one simple tool that does the heavy lifting for you:\\nIt tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.\"]\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-create-benchmark.png\",\n      alt: \"Create a benchmark in Evalry\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-model-comparison.png\",\n      alt: \"Model comparison in Evalry\"\n    }), \"\\n\", _jsx(_components.img, {\n      src: \"/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-results.png\",\n      alt: \"Benchmark results in Evalry\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"I'm also planning to set up continuous monitoring. So when a better model appears, that's cheaper, faster, or higher quality for your use case, you get notified.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So if you're paying for LLM APIs and have never tested alternatives on your actual prompts, you're likely overpaying.\\nGive \", _jsx(_components.a, {\n        href: \"https://evalry.com\",\n        children: \"Evalry\"\n      }), \" a try. It takes 5 minutes to find out if there's a better model for your use case.\\nOr if you're short on time, \", _jsx(_components.a, {\n        href: \"https://evalry.com/models\",\n        children: \"find the model you're currently using\"\n      }), \" and try the five models that have similar performance on average.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true}