{"pageProps":{"post":{"title":"How Benchmarking LLMs on Our Use Case Saved Us $1000+","slug":"benchmarking-llms-on-our-use-case-saved-us-thousands","date":"2026-01-19T11:00:00.000Z","tags":"LLM, AI, Tools, Startups","category":"Projects","description":"We benchmarked 300+ models on our actual task and found a cheaper alternative that works just as well.","type":"text","status":"published","content":"\nThis is the story of how I benchmarked LLMs for a friend of mine, saved him thousands in the process, and built a product around it.\n\nLet me first tell you about my friend. He's an entrepreneur, and a successful one at that.\nAfter he successfully built his last company, a SaaS business, he's now building his next business as a solo founder.\nAlthough his last company was a SaaS startup, he's non-technical. While I cannot go into details, he's re-thinking a traditional business with AI.\nAs part of this business, he's been deploying a lot of prompts to automate processes that have previously been done by humans.\nAnd for that, he picked GPT-5 because, well, it's the default choice. \nIt's solid in most benchmarks, everyone uses it, they never questioned it.\n\nBut as adoption grew, so did the API bill. Hundreds of dollars monthly.\nAs I also do a lot of LLM engineering for my company, he asked me to take a look and help him out.\nAs part of this process, we benchmarked different LLMs against his use cases.\nIt quickly became clear that he can swap out ChatGPT with cheaper alternatives for a fraction of the costs, saving him north of $1000 monthly.\n\n## The Problem: Benchmarks don't predict performance on your specific task\n\nWhen picking an LLM, most people just use choose a model from their favorite provider. \nFor me, that's anthropic, and so depending on the task, I choose Opus, Sonnet, or Haiku.\nSame for OpenAI with a slightly higher variance.\nIf you're sophisticated, you might event check the latest benchmark, e.g. Artificial Analysis or LM Arena. \nOr whatever is the latest hot benchmark for something related to your use case:\nGPQA Diamond, AIME, SWE Bench, MATH 500, Humanity's Last Exam, ARC-AGI, MMMLU...\n\nBut here's the thing: \nNONE of these predict performance on YOUR specific task.\n\nA model that tops reasoning benchmarks might be mediocre at damage cost estimation. \nOr customer support in your customers' native tongue. \nOr spatial reasoning in Germany.\nOr data extraction via playwright. \nOr whatever you're actually building.\n\nThe only way to know is to test on your actual prompts.\n\n## Building benchmarks ourselves\n\nSo we built the benchmarks.\nWe identified the main use cases of prompts, e.g. customer support and cost calculation.\nWe already had the prompts that were used to do this. Sometimes fully automated, sometimes to generate an answer that would then be refined.\nTo create a ground truth for all use cases, we collected as many real-life examples as possible.\nFor customer support, we extracted the actual chats via [WHAPI](https://whapi.cloud/) along with the agents (my friends) answer.\nOr for cost estimation, we collected all the data we had about the contract along with the human-made estimate that got sent out to the customer.\nIf you know a specific model is good enough, you can also simply use its output.\n\nNow we had the context, the pompts, and the expected outputs. \nBut how to now score all the LLMs against each other?\nFor this, we applied the \"LLM as a judge\" approach.\nFor all our samples, we used prompt + context to generate an answer.\nThen we let a (frontier) LLM judge the result based on the given answer of the model we benchmarked.\nThis then gave us a score for every LLM.\nWe also checked the results to make sure they aligned with our subjective scoring.\n\n## Deciding on the best model\n\nNow that we had a score to measure quality per LLM, the question was:\nWhich model should we choose?\nIn practice, you want a model that provides a balance of quality, cost, and latency.\nFor our customer support case, latency was important. We couldn't wait for GPT-5 that took up to a minute with enough context.\nFor price estimation, we wanted the results to be as good as possible for a reasonable price.\n\nThis made us realize, we needed to measure both cost and latency.\n- Cost: For cost, we quickly realized that simply comparing token costs is not enough:\n  Since response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,\n  we decided to simply measure overall cost per answer and average per use case or benchmark.\n- Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.\n  Of course, that differs for chat applications where time to first token, etc. can be essential UX, too.\n\nThis finally gave us a list of models per use case with price, cost, and latency.\nTo decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.\n\nIn theory, there's a concept called Pareto Efficiency that can be applied:\nFor the formal definition, Wikipedia sure does a better job than me.\nFor the informal definition, here's my take:\n\n> Given that you have a benchmark across 100 LLMs with a cost and score (let's forget latency for a second).\n  There's no point in comparing all 100 LLMs.\n  For most LLMs, you will find a model that cheaper AND better.\n  This means there's no point in looking at it, as you have one that's better in both dimensions.\n  Doing this for all LLMs in a benchmark, you get a pareto frontier:\n  The best LLM for a given price.\n  The higher the price, the more quality you get.\n\n## Saving $1000 monthly by switching the models\n\nWith the described method, we found a significantly chepaer model for both use cases.\nAcross the two use cases, this saved him roughly 80% of the monthly costs and over $1000.\n\nSince no technical post ends without a small plug,\nin the following section, I will tell you that I launched this as a small tool.\nStop here if you're not interested.\n\n## evalry: a tool to benchmark your usecase across 300+ LLMs\n\nAs you see, benchmarking and truly finding an optimal model is more complex than we initially though.\nThat's why my friend never did it, that why I unsually don't do it.\nYou need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.\nManually testing even 5 models can quickly become a multi-hour endeavor.\nAnd new models drop weekly. Keeping up is impossible.\nThe same model in a month? Half the price because some LLM wizard like Simon Bohm dropped inference costs in half.\n\nSo to help my friend and anyone else with the same problem, I built [Evalry](https://evalry.com).\nIt provides all this in one simple tool that does the heavy lifting for you:\nIt tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.\nI'm also planning to set up continuous monitoring. So when a better model appears, that's cheaper, faster, or higher quality for your use case, you get notified.\n\nSo if you're paying for LLM APIs and have never tested alternatives on your actual prompts, you're likely overpaying.\nGive [Evalry](https://evalry.com) a try. It takes 5 minutes to find out if there's a better model for your use case.\nOr if you're short on time, find the model you're currently using and try the five models that have similar performance on average.\n"},"mdxSource":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    blockquote: \"blockquote\",\n    h2: \"h2\",\n    li: \"li\",\n    p: \"p\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.p, {\n      children: \"This is the story of how I benchmarked LLMs for a friend of mine, saved him thousands in the process, and built a product around it.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Let me first tell you about my friend. He's an entrepreneur, and a successful one at that.\\nAfter he successfully built his last company, a SaaS business, he's now building his next business as a solo founder.\\nAlthough his last company was a SaaS startup, he's non-technical. While I cannot go into details, he's re-thinking a traditional business with AI.\\nAs part of this business, he's been deploying a lot of prompts to automate processes that have previously been done by humans.\\nAnd for that, he picked GPT-5 because, well, it's the default choice.\\nIt's solid in most benchmarks, everyone uses it, they never questioned it.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But as adoption grew, so did the API bill. Hundreds of dollars monthly.\\nAs I also do a lot of LLM engineering for my company, he asked me to take a look and help him out.\\nAs part of this process, we benchmarked different LLMs against his use cases.\\nIt quickly became clear that he can swap out ChatGPT with cheaper alternatives for a fraction of the costs, saving him north of $1000 monthly.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"The Problem: Benchmarks don't predict performance on your specific task\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When picking an LLM, most people just use choose a model from their favorite provider.\\nFor me, that's anthropic, and so depending on the task, I choose Opus, Sonnet, or Haiku.\\nSame for OpenAI with a slightly higher variance.\\nIf you're sophisticated, you might event check the latest benchmark, e.g. Artificial Analysis or LM Arena.\\nOr whatever is the latest hot benchmark for something related to your use case:\\nGPQA Diamond, AIME, SWE Bench, MATH 500, Humanity's Last Exam, ARC-AGI, MMMLU...\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"But here's the thing:\\nNONE of these predict performance on YOUR specific task.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"A model that tops reasoning benchmarks might be mediocre at damage cost estimation.\\nOr customer support in your customers' native tongue.\\nOr spatial reasoning in Germany.\\nOr data extraction via playwright.\\nOr whatever you're actually building.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The only way to know is to test on your actual prompts.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Building benchmarks ourselves\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So we built the benchmarks.\\nWe identified the main use cases of prompts, e.g. customer support and cost calculation.\\nWe already had the prompts that were used to do this. Sometimes fully automated, sometimes to generate an answer that would then be refined.\\nTo create a ground truth for all use cases, we collected as many real-life examples as possible.\\nFor customer support, we extracted the actual chats via \", _jsx(_components.a, {\n        href: \"https://whapi.cloud/\",\n        children: \"WHAPI\"\n      }), \" along with the agents (my friends) answer.\\nOr for cost estimation, we collected all the data we had about the contract along with the human-made estimate that got sent out to the customer.\\nIf you know a specific model is good enough, you can also simply use its output.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now we had the context, the pompts, and the expected outputs.\\nBut how to now score all the LLMs against each other?\\nFor this, we applied the \\\"LLM as a judge\\\" approach.\\nFor all our samples, we used prompt + context to generate an answer.\\nThen we let a (frontier) LLM judge the result based on the given answer of the model we benchmarked.\\nThis then gave us a score for every LLM.\\nWe also checked the results to make sure they aligned with our subjective scoring.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Deciding on the best model\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Now that we had a score to measure quality per LLM, the question was:\\nWhich model should we choose?\\nIn practice, you want a model that provides a balance of quality, cost, and latency.\\nFor our customer support case, latency was important. We couldn't wait for GPT-5 that took up to a minute with enough context.\\nFor price estimation, we wanted the results to be as good as possible for a reasonable price.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This made us realize, we needed to measure both cost and latency.\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Cost: For cost, we quickly realized that simply comparing token costs is not enough:\\nSince response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,\\nwe decided to simply measure overall cost per answer and average per use case or benchmark.\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.\\nOf course, that differs for chat applications where time to first token, etc. can be essential UX, too.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This finally gave us a list of models per use case with price, cost, and latency.\\nTo decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In theory, there's a concept called Pareto Efficiency that can be applied:\\nFor the formal definition, Wikipedia sure does a better job than me.\\nFor the informal definition, here's my take:\"\n    }), \"\\n\", _jsxs(_components.blockquote, {\n      children: [\"\\n\", _jsx(_components.p, {\n        children: \"Given that you have a benchmark across 100 LLMs with a cost and score (let's forget latency for a second).\\nThere's no point in comparing all 100 LLMs.\\nFor most LLMs, you will find a model that cheaper AND better.\\nThis means there's no point in looking at it, as you have one that's better in both dimensions.\\nDoing this for all LLMs in a benchmark, you get a pareto frontier:\\nThe best LLM for a given price.\\nThe higher the price, the more quality you get.\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"Saving $1000 monthly by switching the models\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"With the described method, we found a significantly chepaer model for both use cases.\\nAcross the two use cases, this saved him roughly 80% of the monthly costs and over $1000.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Since no technical post ends without a small plug,\\nin the following section, I will tell you that I launched this as a small tool.\\nStop here if you're not interested.\"\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"evalry: a tool to benchmark your usecase across 300+ LLMs\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"As you see, benchmarking and truly finding an optimal model is more complex than we initially though.\\nThat's why my friend never did it, that why I unsually don't do it.\\nYou need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.\\nManually testing even 5 models can quickly become a multi-hour endeavor.\\nAnd new models drop weekly. Keeping up is impossible.\\nThe same model in a month? Half the price because some LLM wizard like Simon Bohm dropped inference costs in half.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So to help my friend and anyone else with the same problem, I built \", _jsx(_components.a, {\n        href: \"https://evalry.com\",\n        children: \"Evalry\"\n      }), \".\\nIt provides all this in one simple tool that does the heavy lifting for you:\\nIt tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.\\nI'm also planning to set up continuous monitoring. So when a better model appears, that's cheaper, faster, or higher quality for your use case, you get notified.\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"So if you're paying for LLM APIs and have never tested alternatives on your actual prompts, you're likely overpaying.\\nGive \", _jsx(_components.a, {\n        href: \"https://evalry.com\",\n        children: \"Evalry\"\n      }), \" a try. It takes 5 minutes to find out if there's a better model for your use case.\\nOr if you're short on time, find the model you're currently using and try the five models that have similar performance on average.\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true}