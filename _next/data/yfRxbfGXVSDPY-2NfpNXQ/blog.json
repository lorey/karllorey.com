{"pageProps":{"posts":[{"title":"Without Benchmarking LLMs, You're Likely Overpaying 5-10x","slug":"without-benchmarking-llms-youre-overpaying","date":"2026-01-20T17:35:00.000Z","tags":"LLM, AI, Tools, Startups","category":"Projects","description":"We benchmarked 100+ models on our actual task and found a much cheaper alternative that works just as well.","type":"text","status":"published","content":"\nLast month I helped a friend cut his LLM API bill by 80%.\n\nHe's a non-technical founder building an AI-powered business.\nLike most people, he picked GPT-5 because it's the default:\nYou have the API set up already,\nthe model has solid benchmarks,\neveryone uses it,\nwhy bother?!\n\nBut as usage grew, so did his bill.\n$1,500/month for API calls alone.\n\nSo we benchmarked his actual prompts against 100+ models\nand quickly realized that while GPT-5 is a solid choice,\nit almost never is the cheapest and there are always cheaper options\nwith comparable quality.\nFiguring out which saved him thousands of dollars in the process.\nHere's how we did it.\n\n## The Problem: Benchmarks don't predict performance on your task\n\nWhen picking an LLM, most people just choose a model from their favorite provider.\nFor me, that's Anthropic, so depending on the task, I pick Opus, Sonnet, or Haiku.\nIf you're sophisticated, you might check Artificial Analysis, or LM Arena,\nor whatever benchmark seems relevant:\nGPQA Diamond, AIME, SWE Bench, MATH 500, Humanity's Last Exam, ARC-AGI, MMLU...\n\nBut let's not fool ourselves here:\nnone of these predict performance on your specific task.\nA model that tops reasoning benchmarks might be mediocre at damage cost estimation.\nOr customer support in your customers' native language.\nOr data extraction via Playwright.\nOr whatever you're actually building.\n\nAt best, they're a rough indicator of performance.\nAnd they do not account for costs at all.\n\nThe only way to know is to test on your actual prompts.\nAnd make a decision considering quality, cost, and latency.\n\n## Building benchmarks ourselves\n\nSo to figure this out, we built our own benchmarks.\nLet me walk through one use case: customer support.\n\n### Step 1: Collect real examples\n\nWe extracted actual support chats via [WHAPI](https://whapi.cloud/).\nEach chat gave us the conversation history, the customer's latest message, and the response my friend actually sent.\nMy friend also gave me the prompts he used manually and inside this chat tool to generate responses.\nBased on this, we selected around 50 chats.\nA lot with frequently asked questions, but also some edge cases where we wanted the LLM to behave in a certain way.\n\n### Step 2: Define the expected output\n\nFor each example, we used my friend's actual response as the expected output.\nWe also defined some ranking criteria, for example:\n\n> A good answer tells the customer that this product costs 5.99 and offers to take an order right now.\n\nOr:\n\n> A good answer tells the customer that the return policy gives customers 30 days to send back the order, but that they sent their return over two months after receiving it.\n\nYou get the idea.\n\n### Step 3: Create the benchmark dataset\n\nWe now had a simple dataset:\nthe prompt (conversation + instructions) and the expected response.\n\nAs you see, this is a generic format that could be used for all use cases.\nFor every prompt, you define the expected response.\nIf you know that a specific model works great, you can even use this to generate the response and refine if necessary.\n\n### Step 4: Run all models\n\nWe then ran this dataset across all the LLMs we wanted to benchmark.\nTo make implementation as easy as possible, we chose [OpenRouter](https://openrouter.ai/) to get a broad set of LLMs behind the same API.\nThe beauty of OpenRouter is that you can use the standard OpenAI SDK and just swap out the model name:\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=\"<OPENROUTER_API_KEY>\",\n)\n\ncompletion = client.chat.completions.create(\n  model=\"openai/gpt-5\",  # or \"anthropic/claude-opus-4.5\", \"google/gemini-3-pro-preview\", ...\n  messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\nThis made it trivial to benchmark all models with the same code.\nRunning this across 50+ models gave us a dataframe with:\nprompt, expected response, and actual response per model.\n\nAs you quickly realize, this is more data than you can evaluate manually.\nSo we needed a plan:\nLLMs to the rescue, again.\n\n### Step 5: Scoring with LLM-as-judge\n\nSince manually comparing hundreds of responses is not feasible,\nwe used an [LLM as a judge](https://huggingface.co/learn/cookbook/en/llm_judge).\nFor each sample, we used Opus 4.5 to score how well the actual response matched the expected response on a scale of 1-10.\nThis is why we set very specific criteria in step 2:\nThe LLMs were able to score much more reliably and consistently when given concrete scoring instructions.\nWe also spot-checked a sample of these scores against our own judgment to make sure the judge was reliable.\nFor example, sometimes imprecision in the expected answer led to the \"judge\" model applying scores wrong.\nSo this was more iterative than this list suggests.\nThat's why we prompted for not only scores, but also the reasoning behind them.\n\nWe used the same approach for his other use cases.\nPrompt, expected answer, and then one answer per model along with the judge model's evaluation.\n\n![Dataset creation](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/llm-as-judge.png)\n\n## Deciding on the best model\n\nNow that we had a score to measure quality per LLM, the question was:\nWhich model should we choose?\nIn practice, you want a model that provides a balance of quality, cost, and latency.\nFor our customer support case, latency was important. We couldn't wait for GPT-5 that took up to a minute with enough context, even though it provided great answers.\nIn contrast, for our other use case, damage cost estimation, we wanted the results to be as good as possible for a reasonable cost, however long they take.\n\nThis made us realize, we needed to measure both cost and latency, too.\n\n- Cost: For cost, we quickly realized that simply comparing token costs is not enough:\n  Since response tokens (thinking + actual answer) are costlier and answers varied significantly in token count,\n  we decided to measure overall costs per answer and thus the average costs per use case / benchmark.\n- Latency: Since for both of our use cases the overall time until we get a full response was the only timing-related variable we needed, we used that.\n  Of course, that differs for chat applications where time to first token, etc. can be essential UX, too.\n\nThis finally gave us a list of models per use case with quality, cost, and latency.\nTo decide, it was usually enough to sort by quality and choose a somewhat cheap/fast model.\n\nIn theory, there's a concept called [Pareto Efficiency](https://en.wikipedia.org/wiki/Pareto_front) that can be applied:\nFor a formal definition, the linked Wikipedia article does a better job than me.\nFor the informal definition, here's my take:\n\n> Given that you have a benchmark across 100 LLMs with a cost and score (let's forget latency for a second).\n> There's no point in comparing all 100 LLMs.\n> For most LLMs, you will find a model that is cheaper AND better.\n> This means there's no point in looking at it, as there is another one that's better in both dimensions.\n> Checking this for all LLMs in a benchmark,\n> you get a list of models that have no model that's both cheaper AND better,\n> the Pareto Frontier:\n> The best LLMs for a given price.\n\nHere's my attempt to visualize this:\nI've plotted the price of a model on the x-axis\nand the response quality on the y-axis.\nThe LLMs we benchmarked are the dots.\nFor all models plotted in blue\nthere is no model that's cheaper and better.\nConnecting these gives you the Pareto frontier:\nthe best models for a given price.\n\n![pareto frontier](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/pareto-frontier.png)\n\nLooking at it, it also becomes obvious that looking at other models makes no sense when optimizing for quality and cost.\n\n## Saving $1000 monthly by switching the models\n\nWith these benchmark results, we found models with comparable quality at up to 10x lower cost.\nMy friend chose a more conservative option that still cut costs by 5x, saving him over $1000/month.\n\nThis process was painful enough that I built a tool to automate it.\n\n## evalry: a tool to benchmark your use case across 300+ LLMs\n\nBenchmarking and truly finding an optimal model is more complex than we initially thought.\nThat's why my friend never did it, that's why I usually don't do it.\nYou need to (re-)build all this, integrate multiple APIs, write scoring logic, error logic, etc.\nManually testing even 5 models can quickly become a multi-hour endeavor.\nAnd new models drop weekly. Keeping up is impossible.\nThe same model in a month? Half the price because some [transformer wizard](https://siboehm.com) dropped inference costs in half.\n\nSo to help my friend and anyone else with the same problem, I built [Evalry](https://evalry.com).\nIt provides all this in one simple tool that does the heavy lifting for you:\nIt tests your actual prompts against 300+ models at once. Compare quality, speed, and cost side-by-side. No code required, results in seconds.\n\n![Create a benchmark in Evalry](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-create-benchmark.png)\n\n![Model comparison in Evalry](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-model-comparison.png)\n\n![Benchmark results in Evalry](/img/posts/2026-without-benchmarking-llms-youre-likely-overpaying/evalry-results.png)\n\nI'm also planning to set up continuous monitoring. So when a better model appears, that's cheaper, faster, or higher quality for your use case, you get notified.\n\nSo if you're paying for LLM APIs and have never tested alternatives on your actual prompts, you're likely overpaying.\nGive [Evalry](https://evalry.com) a try. It takes 5 minutes to find out if there's a better model for your use case.\nOr if you're short on time, [find the model you're currently using](https://evalry.com/models) and try the five models that have similar performance on average.\n\nDiscuss: [HN](https://news.ycombinator.com/item?id=46696300) | [X](https://x.com/karllorey/status/2013691168027038056)","folderName":"2026-without-benchmarking-llms-youre-likely-overpaying"},{"title":"Switching from Nikola to Gatsby (static site generators)","slug":"switch-from-nikola-to-gatsby","date":"2022-03-10T16:03:35.000Z","tags":"Python, Lektor, Nikola, Tech, Gatsby, Javascript","category":"Tech","link":null,"description":"Why I switched from Python-based Nikola to JS-based Gatsby with React and Tailwind.","type":"text","content":"\nInterestingly, exactly two years after [migrating from Python-based Lektor to Python-based Nikola](/posts/switch-from-lektor-to-nikola)\nwith this blog, I'm migrating to JS-based Gatsby.\nSince the page was becoming out of date\nand after successfully building [my company's page](https://loreyventures.com) with Gatsby in just a few hours,\nI thought why not try it with this page and a little more data.\n\n## The good\n\nThe conversion of the pages was finished really quickly.\nAlso converting the existing static CSS-based design over to tailwind was fun and made things much more structured\nwhile keeping the layout 95% the same.\nOverall, Gatsby with its static site generation gives you the best of both worlds:\nOne the one side modern frontend capabilities with React, Tailwind, and other modules,\non the other side strong SEO through static site generation.\nFinally, the deployment was done within minutes just by setting up Github Actions.\n\n## The bad\n\nAfter converting layouts and pages, I started with the blog.\nSince Gatsby provides much of its functionality via an GraphQL-based data layer,\nyou need to integrate your existing data into it as well to leverage core functionality.\nAs long as you keep the data simple, that's quite convenient and quickly done,\nbut as soon as you have customized objects,\nit becomes really cumbersome.\nFor example, Lektor supported Markdown frontmatters with the draft status.\nTo replicate this in Gatsby took me more time than necessary by adapting page generation and filtering blog entries.\nAn explanation much better than the one I can provide,\ncan be found at Jared Palmer's [Gatsby vs. NextJS](https://jaredpalmer.com/gatsby-vs-nextjs).\n\n## The ugly\n\nMaybe it is just me being not that experienced in the Javascript world,\nbut I feel the integration of data with GraphQL forces you to repeat filtering logic quickly in different components.\nAlso, that integration basically resulting in a shared data antipattern\nas data is accessed from each component directly.\nWhich will result in having to change several components\nif you change something in the data.\nFor small projects this is not an issue, but for bigger projects I don't see a quick fix.\nAs recommended in the article linked above, I'll look into NextJS next (pun not intended).\n","folderName":"2022-migrating-to-gatsby"},{"title":"Install and run Django completely inside Docker","slug":"install-and-run-django-inside-docker","date":"2020-05-03T11:33:57.000Z","tags":"Django, Docker, Python, Tech","category":"Tech","link":null,"description":"Start a new Django project inside Docker without installing anything on your host.","type":"text","content":"\nThis guide will show you how to install Django inside Docker with docker-compose\nand without installing any dependencies on the host system.\nSince there are a few quirks and I have to look it up myself every damn time,\nhere's a guide hopefully helping me and other the next time.\n\nThis guide will:\n\n- help you install the newest version of django inside your docker container\n- be independent of your host OS and python version, so you can always get the newest version inside docker\n- provide a basic setup of django inside docker with docker-compose to build upon\n\n# Basic requirements file\n\nStart with a requirements.txt file containing only:\n\n```\ndjango\n```\n\nSave it as `requirements.txt`.\n\n# Basic Dockerfile\n\nFirstly, a Dockerfile is needed to set up and run our project.\nWe'll use a basic version you can expand later.\nIt uses Python 3.8, installs all dependencies from your `requirements.txt` file,\nand then copies your project files into the image.\n\nSo copy this in your `Dockerfile`:\n\n```Dockerfile\nFROM python:3.8\n\nWORKDIR /code\n\n# copy and install requirements first\n# -> speeds up build if requirements haven't changed\nCOPY requirements.txt /code/\nRUN pip install -r requirements.txt\n\n# copy rest of files\n# (not needed since we also mount a volume,\n# but you won't mount in production)\nCOPY . /code/\n```\n\n# Basic docker-compose\n\nIn `docker-compose.yml`:\n\n```yaml\nversion: \"3\"\n\nservices:\n  web:\n    build: .\n    volumes:\n      - .:/code/\n    command: bash\n    tty: True\n    ports:\n      # external is the port you use on your host, i.e. localhost:8000\n      # internal is the port django uses inside the container\n      # format: external:internal\n      - \"8000:8000\"\n```\n\n# Running it all\n\nAfter having set up the above files, run the following steps:\n\n1. Run `docker-compose build` to build the images.\n2. Run `docker-compose up` to start the container, leave it running.\n3. Run `docker-compose exec web pip freeze > requirements.txt` to pin the installed dependencies to their actual version. `requirements.txt` should now contain version numbers.\n4. Run `docker-compose exec web django-admin startproject YOURNAME .` (mind the dot!) to start a new django project in the current directory.\n5. Run `sudo chown -R $USER ./` to own the docker-generated files. Otherwise you'll get file permission problems when working with the generated files on your host (outside of docker).\n6. Run `docker-compose exec web python ./manage.py runserver 0.0.0.0:8000`.\n\nYou should now be able to access a congratulations page at `localhost:8000`.\n\n# Next steps\n\nIf you're interested in setting up Django for production,\nfeel free to check out my guide on\n[how to set up Django deployment for production](http://karllorey.com/posts/django-production-docker-mod-wsgi/) for the same stack.\n\n# Note: Changing ports\n\nIf you want to change the port to access your Django application,\nyou only have to change the external port, i.e. the `docker-compose.yml` file.\nSo to switch to port 80, change the line below `ports:` to\n\n```yaml\nports:\n  - \"80:8000\"\n```\n\nThen the container can be accessed with `localhost:80` or just `localhost` (as 80 is the default).\n","folderName":"2020-install-and-run-django-inside-docker"},{"title":"G Suite for your personal domain and email","slug":"g-suite-for-personal-personal-domain-and-email","date":"2020-04-29T10:38:37.000Z","tags":"Google, Hosting, Organization, G Suite","category":"Tech","link":null,"description":"Use Gmail, Calendar, Drive, and other Google tools with your own domain.","type":"text","content":"\nUsing Google's [G Suite](https://gsuite.google.com/) for your personal domain allows you to use your domain with all of Google's tools.\nIt includes Gmail (without ads), Meet, Calendar, Drive with > 30 GB, Docs, etc.\nWhat differentiates it from a regular Google account is that you can use your personal domain and thus your own email, e.g. mail@yourname.com.\nAfter having used a G Suite of my company for a while, I wanted to have it for my personal email and domain, too.\n\n# Benefits\n\nBesides being able to use your personal email for everything and as a Google account, you get:\n\n- event/calendar invites sent to your email will be added to your calendar automatically\n- people can invite you to events/docs/drive with your private email\n- your Android phone has all the Google tools built in and provides a seamless experience\n- and so on, I mean, everybody has a Google account nowadays\n\n# Costs/Pricing\n\nIf you G-Suite use it for your personal domain, you'll be needing one user only.\nSo the price is between 6 and 25 USD per month.\nYou'll be needing the basic version for 6 USD only, I think.\n\n# Getting started\n\nSo I basically signed up for a free trial.\nGoogle asked me for my main email, i.e. the one used for your Google account.\nI choose mail@mydomain.com (say hello if you like :) ).\n\nThere are only a few steps to take and the experience is seamless:\n\n- create a G Suite account\n- set up your [MX records](https://en.wikipedia.org/wiki/MX_record) to point at Google's mailservers\n  (Google provided a tutorial for my registrar automatically)\n- wait a little, and you're ready to go\n- notify contacts of your (new) email\n\n# Set up\n\nPersonally, I didn't change much, but here's what I found useful:\n\n- Gmail: set up a signature, change GMail's layout\n- Calendar: set up default notifications for events 30 and 3 minutes ahead\n  (allows me to get notified early during deep work and just before calls)\n- Drive: Migrate folders/files to your new personal Google Drive.\n  You can do this by sharing files and folders from your old account to your new account.\n\n# Misc\n\nSome random notes:\n\n- somehow you automatically become a business customer and can't change that, which is weird but isn't a problem in my case\n- site verification, a part of the setup process, took a long time. Just give it a few minutes for the DNS records to update.\n","folderName":"2020-g-suite-for-personal-domain-and-email"},{"title":"Add all models to Django admin automatically","slug":"add-all-models-to-django-admin-automatically","date":"2020-03-24T10:01:51.000Z","tags":"Python, Django, Tech, Clean Code","category":"Tech","link":null,"description":"A one-liner that registers all your models with the admin site.","type":"text","content":"\nTo show, edit, and work with a model inside the Django admin site,\nyou usually have to add each desired model manually by adding a line to `{your app}/admin.py`.\nFor example, to add your `Project` class to your Admin site,\nyou have to add the following to your `admin.py` file:\n\n```python\nfrom django.contrib import admin\nfrom myproject.myapp.models import Project\n\nadmin.site.register(Project)\n```\n\nOver time, especially during development when you create new objects a lot,\nthis can become quite tedious.\nSo I cam up with a quick hack to add all models to your Django admin site automatically.\nInstead of registering all classes manually,\nwe simply use inspection to add all classes of our models module.\nThis is what you need to add to the `admin.py` file inside your app:\n\n```python\nimport inspect\n\nfrom django.contrib import admin\n\nfrom myproject.myapp import models\n\nfor name, obj in inspect.getmembers(models):\n    if inspect.isclass(obj):\n        admin.site.register(obj)\n```\n\nThis code uses inspection to automatically add all classes inside your models.\nNote that you cannot have any other classes inside models withough extending the above code.\nI would recommend to use this during development only\nand add all classes manually in production.\n","folderName":"2020-show-all-models-in-django-admin-quickly"},{"title":"Switching from Lektor to Nikola (static site generators)","slug":"switch-from-lektor-to-nikola","date":"2020-03-10T16:03:35.000Z","tags":"Python, Lektor, Nikola, Tech","category":"Tech","link":null,"description":"Why I migrated and what to expect from both Python-based static site generators.","type":"text","status":null,"content":"\nI decided to migrate this website/blog from Lektor to Nikola.\nBoth are static site generators\nimplemented in Python\nthat allow you to generate a static, secure, and maintenance-free website/blog.\nHere is my experience, why I did it, how to do it, and what to expect.\nIn short: While I liked Lektor for its simplicity,\nit lacked several features I needed.\nSo far, Nikola has these features and many more\nwhich is why I'm very happy to have switched.\nRead the rest of the pros and cons here.\n\n# Background: Static Site Generators\n\nBoth Lektor and Nikola are Static Site Generators (SSG) implemented in Python.\nYou basically create everything on your computer and the the generator generates HTML\nwhich can be hosted on GitHub or almost everywhere without any hassle.\nBesides, it's blazing fast, secure, and needs no server configuration/maintenance.\nFind an overview at [Full Stack Python](https://www.fullstackpython.com/static-site-generator.html).\n\n# Lektor vs. Nikola\n\nWhen I created this website, I chose Lektor for it's simplicity.\nCompared to Wordpress, which I had used before, it was easy to set up and maintain.\n\n## Why not Lektor?\n\n- to little default markup: the site did not rank for quite some time which might be due to litte semantic markup\n- it especially misses several essential features to rank and make the site visible\n  - no RSS feed by default which allows search engines to discover new posts quickly\n  - no meta tags which would enable nice previews on social media (OpenGraph for Facebook, etc.)\n- default directory structure: blog posts in Lektor all have an own folder containing a file named contents.lr.\n  While this looks trivial and helps with structuring files belonging to a post, I simply did not like it too much.\n- Lektor does not use regular virtualenv environments which just makes stuff a little trickier to understand\n- the ecosystem is small, there are not many plugins, and development seems to have halted\n\n## Why Nikola?\n\nAgain, this is highly opinionated.\n\n- most-active repository compared to [Lektor](https://github.com/lektor/lektor), [Pelican](https://github.com/getpelican/pelican), [MkDocs](https://github.com/mkdocs/mkdocs/), [mynt](https://github.com/Anomareh/mynt), and other static site generators in Python\n- seamless integration of blog posts and regular pages\n- clear documentation\n- lots(!) of features and configuration options\n- RSS feeds, tags, categories, blog archives, etc.\n- themes with semantic HTML\n\n# Switching from Lektor to Nikola\n\nI basically set up a clean Nikola project and then adapted my old content.\nHere's what I did with a rough time estimation:\n\n- Reading the docs and getting started (30m)\n- Setting up a clean project within my existing repo (30m)\n- Renaming articles and sites (15m)\n- adding Nikola-specific markdown headers (5m per site/article)\n- formatting code blocks to leverage syntax highlighting and fixing markdown errors (30m)\n- integrating my existing theme into Nikola (2h)\n- setting up deployment on Travis CI (30m)\n\n# What I like so far\n\n- a lot of functionality with sane defaults\n- whole transition took me less than one day, no weird errors, no frustration\n- clean structure that enables (even hierarchical) posts and pages without any issues\n- super fast deployment setup with github (took me one minute)\n- great default/base templates with mako or jinja as a templating engine that can be adapted really easily\n\n# What I don't like so far\n\n- The config file is huge and includes a lot of documentation, basically very similar to Django.\n  While I like to configure stuff, it's hard to find settings and understand all implications.\n- Lektor has an amazing concept to define own models, I haven't figured out how to do something similar in Nikola, e.g. for my [CV](/founder).\n- There are not many templates available. I've also found my template to be quite messy and without clean syntax.\n- There's a plugin system heavily relying on [doit](https://pydoit.org/) which makes site-creation very efficient but extending as a result rather difficult to understand. For example, I don't know how to insert a list of all subpages into the the current page which should be quite easy.\n\n# Issues\n\nIf you're also getting started, here are the issues I ran into.\nAs I said, not many and not frustrating :)\n\n## Overwriting themes\n\nWhile there are a few basic and advanced themes you can build upon,\nit took me quite some time to find out where these themes can be found.\nThis is necessary to understand which files there are and which files you need to overwrite to adapt a theme.\nBasically most templates you can inherit from can be found at [nikola-themes](https://github.com/getnikola/nikola-themes)\nbut some basic templates are stored within [Nikola itself](https://github.com/getnikola/nikola/tree/c886ea38f7ee34f1fffb3edae7087694483a999d/nikola/data/themes).\nIf you've understood this, you basically just copy/paste the files you want to adapt inside your own theme\nwhich works really nice.\n\n## Deployment with existing gh-pages\n\nSince I had previously set up Lektor deployment via GitHub Pages, I already had a `gh-pages` branch.\nSo when I tried to deploy via `nikola github_deploy`, I got the following error:\n\n```text\nTo github.com:lorey/karllorey.com.git\n ! [rejected]        gh-pages -> gh-pages (fetch first)\nerror: failed to push some refs to 'git@github.com:lorey/karllorey.com.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\n# .stacktrace\n[2020-02-25T15:12:28Z] ERROR: github_deploy: Failed GitHub deployment -- command ['ghp-import', '-n', '-m', 'Nikola auto commit.\\n\\nSource commit: 5d74f162affaf80691de1d7c7b0d8728addea96a\\nNikola version: 8.0.4', '-p', '-r', 'origin', '-b', 'gh-pages', 'output'] returned 1\n```\n\nObviously, I forgot to pull the gh-pages branch first, so the new commit could be added on top.\nAfter doing a `git pull origin gh-pages`, deploying worked like a charm.\n\n## Deployment with Travis CI\n\nTo deploy with Travis CI, there's a great [post on the Nikola blog](https://getnikola.com/blog/automating-nikola-rebuilds-with-travis-ci.html).\nWhat I did not understand at first was the distinction between the `master` and a `src` branch.\nSo the `src` branch is used for user pages that are deployed into the master branch, i.e. when you have a site pointing to you username, e.g. `lorey.github.io`.\nIf you do that, you have to use another branch to deploy to avoid an infinite loop (push to master > deploy to master > ...).\nSince I use `gh-pages` as the branch containing all the generated HTML, I set up .travis.yml to deploy on pushes to master.\nIn the `conf.py` I still set:\n\n```python\nGITHUB_SOURCE_BRANCH = 'src'\n```\n\nSeems to work fine.\n\nAnother thing I encountered when setting up Travis CI:\nSomehow the travis gem had issues when logging in so `travis login` and `travis enable` did not work.\nMaybe because I use an old version or have 2FA enabled.\nIn the end I just ran `travis encrypt-file id_rsa --add` which worked fine and added a line to .travis.yml\nstoring the encrypted key.\n\nDoing a `git branch master FETCH_HEAD` inside the .travis.yml failed:\n\n```text\n$ git branch master FETCH_HEAD\nfatal: A branch named 'master' already exists.\nThe command \"git branch master FETCH_HEAD\" failed and exited with 128 during .\n```\n","folderName":"2020-switch-from-lektor-to-nikola"},{"title":"Django in Production with mod_wsgi and Docker","slug":"django-production-docker-mod-wsgi","date":"2020-01-17T09:30:13.000Z","tags":"Django, Python, Docker, Tech","category":"Tech","link":null,"description":"Run Django with apache2/mod_wsgi in Docker with minimal configuration.","type":"text","content":"\nThis article will show you how to run Django in production with docker and apache2/mod_wsgi.\nWhile there are several guides on how to do it,\nI found no simple enough tutorial on how to do it\nsince all of the existing solutions require a lot of configuration or a custom docker image.\nThe solution I found is way quicker and requires close to no configuration.\n\nGuides I found:\n\n- [Django with Apache and mod_wsgi](https://docs.djangoproject.com/en/3.0/howto/deployment/wsgi/modwsgi/) assumes you have installed everything already and requires configuration possibly unnecessary for a dockerized version\n- [mod_wsgi](https://modwsgi.readthedocs.io/en/develop/) explains how to install mod_wsgi, but not how to use Django with it\n\nSo, let's do this.\nIf you just want to see the final Dockerfile, scroll to the end of the article :)\n\n## Prerequisites\n\nI assume you have some Dockerfile with Django application setup\nand run Django via the command that spins up the development server:\n\n```dockerfile\nFROM python:3.6\n\nWORKDIR /code/\n\n# copy and install requirements first to leverage caching\nCOPY requirements.txt /code/\nRUN pip install -r requirements.txt\n\n# copy the actual code\nCOPY . /code/\n\nCMD ./manage.py runserver 0.0.0.0:8000\n```\n\n## Step 1: Install Apache\n\nWe basically need a working Apache setup to run python code via mod_wsgi.\nSo in our Dockerfile, we need to install apache2 and apache2-dev.\n\n```dockerfile\nRUN apt-get install apache2 apache2-dev\n```\n\n## Step 2: install mod_wsgi\n\nWe want to run our project within a container.\nThe mod_wsgi documentation says the easiest and preferred using docker\nis to use mod_wsgi-express as it does not require any configuration.\nmod_wsgi-express can be used as a command after you have installed it via pip.\nSo we need to add:\n\n```dockerfile\nRUN pip install mod_wsgi\n```\n\n## Step 3: run Django inside Apache with mod_wsgi\n\nLastly, we simply have to run mod_wsgi-express\nwhich will start an apache instance with our Django project.\nThis can be achieved via:\n\n```text\nmod_wsgi-express start-server /code/project_name/wsgi.py --user www-data --group www-data\n```\n\nThe wsgi.py file is auto-generated by Django's `startproject` command and inside you app's folder,\ni.e. `django-project/project_name`.\nSo, to run our project within Apache after we've set-up everything in the Dockerfile,\nwe have to add a `CMD` command at the end of the Dockerfile:\n\n```dockerfile\nCMD mod_wsgi-express start-server /code/connect_web/wsgi.py --user www-data --group www-data\n```\n\nThe `--user` and `--group` parameters make sure Apache isn't run as root which results in errors.\nIf you want to find out, just run the command without them.\n\n## Final Dockerfile: Installing and running Apache and mod_wsgi within Docker\n\nYour final Dockerfile should now look like this:\n\n```dockerfile\nFROM python:3.6\n\n# update packages\nRUN apt-get -qq update\nRUN apt-get install --yes apache2 apache2-dev\nRUN pip install mod_wsgi\n\nRUN mkdir /code\nWORKDIR /code\n\nCOPY . /code/\n\nCMD mod_wsgi-express start-server /code/project_name/wsgi.py --user www-data --group www-data\n```\n\n## Troubleshooting\n\nIf you have any problems, check the logs. Their locations are outputted when starting the container:\n\n```text\nweb_1     | Server URL         : http://localhost:8000/\nweb_1     | Server Root        : /tmp/mod_wsgi-localhost:8000:0\nweb_1     | Server Conf        : /tmp/mod_wsgi-localhost:8000:0/httpd.conf\nweb_1     | Error Log File     : /tmp/mod_wsgi-localhost:8000:0/error_log (warn)\nweb_1     | Request Capacity   : 5 (1 process * 5 threads)\nweb_1     | Request Timeout    : 60 (seconds)\nweb_1     | Startup Timeout    : 15 (seconds)\nweb_1     | Queue Backlog      : 100 (connections)\nweb_1     | Queue Timeout      : 45 (seconds)\nweb_1     | Server Capacity    : 20 (event/worker), 20 (prefork)\nweb_1     | Server Backlog     : 500 (connections)\nweb_1     | Locale Setting     : en_US.UTF-8\n```\n\nIf you want to monitor them, just `tail -f` them:\n\n```text\ntail -f /tmp/mod_wsgi-localhost:8000:0/error_log\n```\n\n### Permission problems\n\nAt first, I got an `Internal Server Error` when opening the page on my machine.\nAfter checking the logs as described above, mod_wsgi seemed to have problems with file permissions on `.logs/debug.log`.\nA simple `chown www-data /code/.logs/debug.log` from within the container\nwhich makes www-data the owner of the file solved it for me.\n","folderName":"2019-django-production-docker-mod-wsgi"},{"title":"How to recover an AWS EC2 instance without the private key","slug":"recover-aws-ec2-instance-without-private-key","date":"2019-09-03T09:30:13.000Z","tags":"AWS, Tech","category":"Tech","link":null,"description":"Mount the volume in a temporary instance and replace the SSH key.","type":"text","content":"\nLost the private key for your EC2 instance and can't login via ssh anymore?\nThis tutorial will show you how to recover your EC2 instance by setting a new key pair to login.\n\nWhat we'll do:\n\n- mount the original instance's volume (a.k.a. it's filesystem) inside another temporary EC2 instance\n- modify the keys allowed to login\n- unmount the volume from the temporary instance and re-mount it in the original instance\n- login with a new key to your original instance\n\nIn short, this replaces the key needed for ssh to connect with a new one of your choice.\n\n## AWS Recovery Automation\n\nThere's an [Amazon recovery automation thing](https://aws.amazon.com/premiumsupport/knowledge-center/recover-access-lost-key-pair/) available that aims to recover your instance automatically,\nit sadly did not work for me.\nSo here we go.\n\n## Step 1: Find the instance\n\nGo to the AWS EC2 console and find your (lost) instance.\nMake sure you're in the right availability zone.\nNote down the instance ID as well as the subnet.\nAlso note the instance's volume\n\n## Step 2: Create a temporary instance\n\nLaunch an instance in the same availability zone.\nMake sure to use the same subnet.\nCreate a new key pair with that instance\nor use the key pair you'd like to use for your original instance from now on.\n\n## Step 3: Attach the original volume to a temporary instance\n\nStop the original instance to be able to unmount the storage.\nNote the volume ID under `Attachment information`.\nGo to volumes and detach the volume with `Actions - Detach Volume`.\nAttach the volume to the temporary instance with `Actions - Attach Volume`.\nChoose one of the given options and note it down, e.g. `/dev/sdf`.\nConnect to the temporary instance via SSH and mount the volume, e.g. to `/data` via the following command:\n\n```text\nmount /dev/sdf /data\n```\n\nThe volume of the original instance is now mounted to `/data`.\nThis allows us to now modify the allowed keys.\n\n## Step 4: Modify the allowed keys\n\nWe can now set the key of the current, temporary instance as an allowed key of the original instance.\nThe keys allowed to log in are stored in a file called `~/.ssh/authorized_keys` (background on [authorized_keys](https://www.ssh.com/ssh/authorized_keys/)).\nInside this file is just a line-by-line list of authorized keys.\nBecause of this, we can just append the file of our temporary instance (and thus our key from the temporary instance)\nto the file of our original instance.\n\n```text\ncat ~/.ssh/authorized_keys >> /data/home/admin/.ssh/authorized_keys\n```\n\nMake sure to swap admin with the actual user you want to sign in as (check your ssh connection command if you're unsure).\n\n### Check if everything went right\n\n```text\ncat /data/home/admin/.ssh/authorized_keys\n```\n\nshould now contain the contents of:\n\n```text\ncat ~/.ssh/authorized_keys\n```\n\n## Step 4: Bring everything back in order\n\nWe now have set up another key for login.\nAll that's left is to unmount the volume from the temporary instance and mount it to the original instance.\n\nFirst, we have to unmout the storage of the original instance inside the temporary instance by\n`umount /dev/sdf` (make sure to use the right path here).\nAfterwards, you stop the temporary instance via the AWS console under `Instances`.\nYou then attach the volume to the original instance inside the AWS console under `Volumes` via `Actions - Attach volume`.\nType in the original instance ID as well as `xvda` as the mount point.\nOtherwise, you might get an error pointing out that there's no root volume when starting the instance.\nYou can now re-start the original instance and should be able to login with the new key.\n\nMake sure to delete the temporary instance in case everything went well.\n\n## Conclusion\n\nSo this guide showed you how to recover an AWS EC2 instance if you lose you private key.\nWe did this by using a temporary instance to swap or actually extend the authorized_keys file.\nYou should now be able to login to the original instance with your new key.\n\nThere's also a slightly different [guide by AWS](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-lost-key-pair).\n","folderName":"2019-recover-aws-ec2-instance-without-private-key"},{"title":"How to set up Black with Debian, PyCharm, and IdeaVim","slug":"set-up-black-pycharm-ideavim","date":"2019-07-28T09:30:13.000Z","tags":"Python, Black, Docker, Pycharm, IdeaVim, Tech","category":"Tech","link":null,"description":"Run the Python formatter in Docker and integrate it with your IDE.","type":"text","content":"\nI've come to like the code formatter [Black](https://black.readthedocs.io/en/stable/index.html) for Python.\nIt's opinionated, deterministic and thus very minimalistic.\nAnd since I'm using it on more and more projects, I wanted to integrate it into my workflow.\n\nSo usually, you just install Black globally via `pip3 install black`.\nBut since I use Debian which still ships with Python 3.5\nthis yielded some smaller challenges as Black only runs under Python 3.6+.\nThis post is a small tutorial on how you can use Black on Debian and integrate it into the command line, PyCharm, and IdeaVim.\n\n## Challenge 1: Python 3.6+ on Debian\n\nSince Debian currently ships with Python 3.5, I needed to get Python 3.6 running somehow.\nWhile installing [Anaconda](https://www.anaconda.com/distribution/) or [Python 3.6+ manually](https://www.python.org/downloads/)\nare other solutions [might](https://community.hortonworks.com/idea/212478/independent-python-vs-anaconda-python.html) [work](https://unix.stackexchange.com/q/332641),\nI decided to simply use Docker, as this is what I use for my regular development anyhow.\nSimplest solution was to use an existing docker image,\nnamely [jbbarth's docker-black](https://github.com/jbbarth/docker-black),\nwhich allows you to mount the current working directory into a (newly created) container, format the desired file, and throw the container away afterwards.\nSounds more complicated than it is, you just run one single command et voila.\nThis method has the additional benefit that it adheres to any [pyproject.toml](https://github.com/psf/black#pyprojecttoml) which can store configuration like line length.\nThere's also a more sleek image at [cytopia/docker-black](https://github.com/cytopia/docker-black) which might take less space.\n\nSo to run black irrespective of your local Python installation via `black main.py`,\nyou run\n\n```text\ndocker run --rm -v $(pwd):/code jbbarth/black main.py\n```\n\nThis creates a new container, mounts the current working directory into `/code` and formats `main.py`.\nAfterwards, the `--rm` flag will delete the container as well as the created volume.\nSo far, so good, but as this command is quote long, I had to build an alias to invoke it quickly as a next step.\n\n## Challenge 2: Black from command line\n\nSo to avoid the cumbersome docker syntax each time, you now want an alias to just run `black main.py` everywhere.\nTo do this, you have to map the black command to your black running inside docker.\nYou can do this by adding\n\n```text\nblack() { docker run --rm -v $(pwd):/code jbbarth/black $*; }\n```\n\nto your bashrc or zshrc.\nAfter opening a new terminal, you should now be able to invoke the black formatter inside docker by running `black ...`.\n\n## Challenge 3: Black in PyCharm\n\nTo now integrate this setup into PyCharm,\nyou have to slightly adapt the [offical installation instructions](https://black.readthedocs.io/en/stable/editor_integration.html#pycharm-intellij-idea).\nGo to `File -> Settings -> Tools -> External Tools`.\nClick the + icon to add a new external tool with the following values:\n\n- Name: `Black`\n- Description: `Black code formatter`\n- Program: `/usr/bin/docker`\n- Arguments: `run --rm -v $FilePath$:/$FilePath$ jbbarth/black \"$FilePath$\"`\n- Working directory: `$ProjectFileDir$`\n\nTest it by running it with an opened python file via `Tools -> External Tools -> Black`.\nAfter you made sure it works, re-open it again and untick `open console` to avoid a new console at every run.\nYou can basically mirror this guide to install a file watcher\nthat formats on every save (see [the docs](https://black.readthedocs.io/en/stable/editor_integration.html#pycharm-intellij-idea)).\n\nNote: As you can see, this only mounts the current file and thus does not adhere to any config files.\nThe same applies for the next step as it builds upon this one.\n\n## Challenge 4: Black in IdeaVim\n\nNow for the bonus part: to trigger this setup quickly from within IdeaVim and format the current file with a single command,\nequivalent to the [regular vim plugin](https://black.readthedocs.io/en/stable/editor_integration.html#vim),\nwe have to map the `:Black` command to our external command in PyCharm.\nTo do this, we edit our `.ideavimrc` file where all IdeaVim configuration is stored,\nand add the following line:\n\n```text\ncommand Black action Tool_External Tools_Black\n```\n\nNow typing `:Black` will re-format the current file.\n\n## Summary\n\nThis tutorial showed you one way to include Black into you daily development (esp. on Debian).\nIf you have any questions or feedback, hit me up on Twitter [@karllorey](https://twitter.com/karllorey)\nor any of the other platforms listed below.\n\n## Notes\n\n### Deleting all docker containers of a specific image\n\nIf you want to remove all containers derived from a specific image, e.g. if you forgot to add the --rm flag:\n\n```text\ndocker rm $(docker ps -a --filter ancestor=jbbarth/black -q)\n```\n","folderName":"2019-set-up-black-pycharm-ideavim"},{"title":"Keeping Pandas DataFrames clean when importing JSON (with Context Managers)","slug":"keeping-pandas-dataframes-clean-importing-json","date":"2019-03-03T09:30:13.000Z","tags":"Machine Learning, Pandas, Clean Code, Python, Tech","category":"Tech","link":null,"description":"Use a Python Context Manager to automatically clean up temporary columns.","type":"text","content":"\nFor work, I do a lot of data analysis to find the most promising young startups.\nAs a first step, you always have to import the desired data into a Pandas DataFrame\nand do some preprocessing, for example by importing JSON data from some API.Â \nWhen doing this kind of pre-processing,\nyou usually have a lot of temporary columns in your DataFrame that get imported but need to be dropped later in the process.\nTo deal with these temporary columns,\nI built a custom Context Manager that keeps track of all imported columns\nand deletes them when you're done.\nThis way, your code stays lean and you don't have to remove temporary columns yourself.\nIn this short article, I will show how you can keep your pre-processing clean\nand use a Python ContextManager to clean up temporary columns.\n\nIn this example I will use the actual code I use for importing data from the API of our CRM named Hubspot.\nWhat I retrieve is a list of companies stored as a list of Python dictionaries.\nTo import a list of dictionaries in pandas you basically do:\n\n```python\nfrom pandas.io.json import json_normalize\n\ndf = json_normalize(data)\n```\n\nThe json_normalize function generates a clean DataFrame based on the given `data` parameter and normalizes the hierarchy so you get clean column names.\nThis is especially useful for nested dictionaries.\n\n## Ugly: Keeping imported columns\n\nThe problem with json_normalize is that you usually only want a subset of the imported columns,\nmostly with different names or some kind of pre-processing, too.\nSo you might be tempted to do something like this:\n\n```python\nfrom pandas.io.json import json_normalize\n\ndf = json_normalize(data)\n\ndf['company_id'] = df['companyId']\ndf['location'] = df['properties.city.value']\ndf['name'] = df['properties.name.value']\ndf['domain'] = df['properties.website.value']\n//..apply(), .as_type(int), whatever...\n```\n\nThis works, but keeps all the imported columns inplace and might take a lot of storage.\nSo what can you do?\n\n## Ugly: Dropping columns manually\n\nSo after importing, you want to get rid of all temporary columns from the import.\nTo do this, you have to either select the columns you want or drop all columns you don't want.\nIn both cases, you have to somehow keep track of the temporary columns or the ones you want to keep.\nTo deal with this, one solution would be to prefix temporary columns and delete them afterwards:\n\n```python\nfrom pandas.io.json import json_normalize\n\ndf = json_normalize(data)\n\n// make temporary columns\ndf.columns = ['temp_' + c for c in df.columns]\n\n// pre-processing, basic calculations, etc.\ndf['company_id'] = df['temp_companyId']\ndf['location'] = df['temp_properties.city.value']\ndf['name'] = df['temp_properties.name.value']\ndf['domain'] = df['temp_properties.website.value']\n//..apply(), .as_type(int), whatever...\n```\n\nAfterwards, you would then select all desired columns or drop all undesired columns.\n\n```python\ndf.drop([c for c in df.columns if c.startswith('temp_')], axis=1, inplace=True)\n// or\ndf = df[[c for c in df.columns if not c.startswith('temp_')]]\n```\n\nWhile this works, it feels bloated and inefficient.\nYou have to prefix all the value names in the code which results in bloated column names.\nYou also have to keep track of column names you want in the end\nor the used prefix in different places.\nJust imagine you have to change the prefix `temp_` one day or make the code work with a different prefix.\n\n## Clean and easy: using a Context Manager\n\nAfter having used the above methods for some time, it struck me that [Python Context Managers](https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/) might be a cleaner solution.\nYou might know them from their most popular application `with open() as file:`.\nIf not, please take a few minutes to read more about them.\nTo make things short: They basically ensure that something, usually a cleanup, is executed in each exit scenario,\nwhether it is a usual exit like a return or an exception.\nI thought I might use this to build a clean solution that keeps track and gets rid of temporary columns.\nSo I built a Context Manager that deals with temporary columns when importing JSON data so I don't have to.\nYou can basically use it like this:\n\n```python\nwith DataFrameFromDict(companies) as df:\n    // imported dict now in df, same result as json_normalize\n    df['company_id'] = df['companyId']\n    df['location'] = df['properties.city.value']\n    df['name'] = df['properties.name.value']\n    df['domain'] = df['properties.website.value']\n// after context exits, df contains company_id, location, name, and domain\n// but no more temporary columns\nprint(df)\n```\n\nThe benefit: You don't have to keep track anymore and the context manager handles the deletion of all temporary columns.\n\n## How it works\n\nYou can just copy and paste the following snippet to get going, I'll explain how it works below:\n\n```python\nclass DataFrameFromDict(object):\n    \"\"\"\n    Temporarily imports data frame columns and deletes them afterwards.\n    \"\"\"\n\n    def __init__(self, data):\n        self.df = json_normalize(data)\n        self.columns = list(self.df.columns.values)\n\n    def __enter__(self):\n        return self.df\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.df.drop([c for c in self.columns], axis=1, inplace=True)\n```\n\nWhen opening the context, `__init__` and `__enter__` get called.\nThey create the DataFrame and remember all imported and thus temporary column names.\nWhen the context is exited, `__exit__` makes sure to drop all previously created columns\nand leaves only the newly created columns behind.\n\nHope this helps you to create a clean pre-processing pipeline.\nLet me know what you think.\nYou can find the [code on GitHub](https://gist.github.com/lorey/2b57b4ebfec4d45221e15a49060f80d2).\n\nFurther reading:\n\n- [json_normalize](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.io.json.json_normalize.html)\n- [Python Context Managers](https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/)\n","folderName":"2019-keeping-pandas-dataframes-clean-importing-json"},{"title":"Founding a Venture Capital Fund as a Techie: My Year 2018","slug":"year-in-review-2018","date":"2019-01-03T09:30:13.000Z","tags":"Entrepreneurship, Year in Review, Venture Capital","category":"Year in Review","link":null,"description":"Reflections on founding First Momentum and doing our first investments.","type":"text","content":"\nI have long been thinking about writing a summary of my year\nto reflect on things and to be able to see how I saw the world in a few years.\nAlso, if you haven't heard of me in 2018, this is the one article you should probably read.\n\nAs you probably know, I founded a venture capital fund, First Momentum, at the age of 27 in late 2017âat a point when I had not even finished university.\nGladly, I graduated last year while getting the fund running, so this year was all about getting traction.\nMost notably this year, we managed to do our first closing and could since do our first investments.\nPersonally, I started to lead the investing team at First Momentum, currently consisting of five people.\n\nSo here are my reflections on this year, my learnings, and my goals for 2019.\n\n## Overview of my 2018\n\nTo keep things short, I will use a list to give you an overview of my 2018:\n\n- After founding the fund in 2017, we got registered with BaFin at the beginning of 2018, meaning we passed at regulatory checks. A huge milestone.\n- So it was all about fundraising as quick as possible afterwards. Making call, having meetings, presenting at events.\n- In June, we were able to to our First Closing. For funds, this is the kick-off and enabled us to actually invest for the first time.\n- Quickly afterwards, we announced our first investment in August.\n- After the first closing, I switched from fundraising over to investing and started to lead the investing team.\n- We managed to grow our investment funnel radically until the end of the year and will announce several investments soon.\n\n## My Challanges\n\nTo start the actual review of 2018, I will begin with my main challenges this year.\n\n### Callenge 1: less tech, more management\n\nFirst and foremost, the thing that was really hard this year was the switch from tech to management.\nSince I've mostly had CTO or programmer roles before, I always dealt with a lot of tech.\nThis year, when building the fund I mostly did fundraising, consisting of calls and meetings.\nLater on, when looking for our first investments, I had to build a team, manage it, and still do meetings and calls.\nWhile I like that very much, too, my passion is to build technology.\nAnd most importantly, coding just feels like my personal ikigai.\nIt doesn't bother me to be coding and thus working 16 hours a day.\nI don't get tired, I don't get bored, and I feel extremely satisfied in the evening.\nEven after 16 hours of coding, I have to push myself to stop and go sleeping to be able to retain my run the next day.\nWith other work, especially management, it just does not feel the same.\nYou don't single-task one thing, you're multitasking >10 things a day.\nA while I was able to maintain a solid workload without much coding this year,\nI just miss the feeling of complete focus and being in the zone for extended periods of time.\n\n### Challenge 2: letting fires burn\n\nThe main challenge at First Momentum has been the constant struggle to do the right thing.\nThere are always more things you should be fixing than you can actually fix.\nReid Hoffman's Masters of Scale podcast even has [an own episode](https://www.reidhoffman.org/why-the-best-entrepreneurs-let-fires-burn/) on it.\nIt can be compared with several fires burning around you at once.\nWhile you natural reaction would be to try to extinguish all fires,\nyour best reaction is to assess the fires and only deal with the ones, that will kill you within the next month,\nso you still have time to think about your actual strategy.\nIn reality, there are so many distractions and seemingly urgent things that shoot at you every day.\nUsually, your reaction would be to react and deal with everything (put out fires).\nWhat you actually should be doing is assessing their importance and to say NO more often (letting fires burn).\nIn conclusion, the challenge often was to act instead of react and thus have and keep a clear strategy.\nThis takes practice, a lot of effort, and you have to remind yourself over and over again.\n\n### Challenge 3: ageing, staying healthy\n\nMost of people reading this will probably laugh.\nBut age has been an important aspect of my life this year.\nNot only was it the first year I have actually noticed to age,\nbut it was also an the first year not being a student anymore which also makes me feel older.\nOn top of that, I'm the second-oldest at First Momentum and get reminded from time to time.\nFurthermore, its quite a challenge to stay healthy when working more than 60 hours over extended periods of time.\nTo deal with age and health, I've started to do more sports, eat healthy, and sleep more.\nNot that I did not do it before, I now do it with more purpose and not only just for fun when I feel like it.\n\n### Challenge 4: staying social when stakes are high\n\nBecause of the workload this year, staying social has been quite a challenge.\nI mean, its easy to be social during university:\ndaily classes, lots of parties, enough time to socialize.\nBut since the workload has risen this year,\nI have to actually focus on keeping in touch with friends for the first time in my life.\nIt's just not as easy to have a beer or make a call after a long work-day.\n\n## My Learnings\n\n2018 has not only been about challenges of course.\nWhen founding a startup, it's basically all about learning, fast!\nSo here are my top learnings this year:\n\n### Learning 1: say no. let fires burn.\n\nAs a founder, your first priority is to act strategically.\nTo do this, you need to have time, be relaxed, and focus.\nLike mentioned in my 2nd challenge this year, distractions are everywhere:\nevents with pitches or networking, people asking you for calls and meetings, startup competitions, you name it.\nWhile not all of these activities are a waste of time in general, they can become one quickly.\nThe one main question to ask yourself is: what is the best way to spend these eight hours?\nIs it really the conference?\nOr rather a one-on-one with a key employee and six calls with excellent founders you already know?\nSo this year was all about learning to say no.\nSaying no to pitches where our target audiences (founders or investors) are not present.\nSaying no to calls and meetings without a clear purpose or benefit.\nAnd saying no to startup competitions without huge monetary or marketing benefits.\nThis way, you will have a calendar with much more space to focus on the actually important things.\n\nIf you'd like to learn more:\nThere's a great article on [Why productive people have empty schedules](https://www.fastcompany.com/3009536/why-productive-people-have-empty-schedules) including Warren Buffet.\nTim Ferris also has two great episodes on saying no ([#282](https://tim.blog/2017/11/25/how-to-say-no/) and [#328](https://tim.blog/2018/07/19/essentialism/))\n\n### Learning 2: focus on what's really important\n\nSo after having cleared your schedule by saying no more often and by letting fires burn the question remains:\nWhat is the most important thing I should be working on?\nI don't know.\nAnd I'm often still not sure.\nI guess nobody launching a startup is and can be sure because of the intrinsic uncertainty.\nBut what I learned in 2018 is that you can learn to focus on what's really important.\nI basically started to take more time to re-think our status quo, week after week.\nIf you hear it, it may sound obvious, but things get ugly when you have a tight schedule, an unbearable workload, or deadlines to meet.\nBut still: nothing is more important than to reflect and to re-adjust your strategy.\nFor these strategy sessions, these are some of the questions I think about:\n\n- what brings you one step closer to your vision?\n- what is the task with the most leverage?\n- which low-risk experiment could yield huge wins?\n- which are the things that take most of your time? could they be more efficient? could they be replaced or even left out?\n- what is the one hurdle that makes your life hard right now?\n\nBeside these questions, here are some topics I learned to focus on more:\n\n- continuous self-improvement: your priority is to grow\n- thinking in long-term games: what can you do today to win in one year?\n- startup culture: will keep your team together in tough times\n- learning from others (podcasts, articles, mentors): others had your problems before and already solved them.\n\nNaval Ravikant has great tweetstorms on stuff like this, for example [How to Get Rich (without getting lucky)](https://twitter.com/naval/status/1002103360646823936).\nThere's also a great [overview article on Medium](https://medium.com/@noahmadden/navalism-quotes-perceptions-by-naval-ravikant-a5fd60ac5788).\nIf you want to learn more about strategy and deriving tasks, I can highly recommend to look into OKRs and everything around them.\nAlso there's is a great episode on [Masters of Scale: How to kill your bad ideas](https://mastersofscale.com/#/mark-pincus-how-to-kill-your-bad-ideas-masters-of-scale-podcast/).\nAnd my friend Feliks has written a great article on [strategy vs. execution](https://medium.com/@feliks/the-biggest-startup-strategy-myth-and-how-to-deal-with-it-455b034e93e8).\n\n### Learning 3: growing your team is hard work\n\nMy company, First Momentum, grew from five founders to twelve employees this year.\nWhile this might not seem a lot, it is hard work.\nFirstly, you have to make sure your company culture stays on track.\nWhile the founders may have a common sense of what the company stands for, new employees don't und you thus have to make sure you communicate these values well and often.\nSecondly, the amount of work required to manage a team is underestimated.\nYou have to guide employees, explain things, and simply catch up with the emotionally from time to time.\nEspecially when new employees join the team, your deep work as a founder is reduced drastically.\n\n### Learning 4: others had your problems before\n\nIt's easy to bury one's head in the sand when working hard and to forget to look left and right.\nBut in fact, you should do the opposite:\nMany other founders had the same problems as you before.\nSo you can either try to come up with a solution yourself which takes a lot of time and effort.\nOr you can simply approach others and get feedback.\nThe natural tendency is to come up with an own solution,\nbut the smart solution is to stand on the shoulders of giants and learn from others.\n\n## My Achievements\n\n- Made our first closing along with national press coverage\n- Made our first investments\n- formed a team of outstanding people. established culture and onboarding.\n- competitive technological advantage. scrapers and reporting.\n- released my first OSS library and re-launched my personal website\n- founded my own holding company\n\n## 2019 by the numbers\n\n- sleep\n- work\n- challenges\n\n## Miscellaneous\n\n- neglecting my laptop.\n- moving to the office full-time\n- making extensive use of a notebook helps a lot.\n- best hike of my life.\n- pitching one billionaire\n- closing a deal at the first call\n- best talk this year at Let's Hack in MÃ¼nster\n- started to form an awesome team\n- buying a new bike and doing more sports to relax\n- part of jury with frank thelen.\n- first year I'm not studying (part-time) anymore\n\nPlatforms, Services and Apps I'm using regularly\n\n- Habits: tracking my habits (daily)\n- Libra: tracking my weight (daily)\n- Sleep for Android: tracking my sleep (daily)\n- Instagram: keeping in touch with friends, getting inspired (daily)\n- Twitter: getting news, networking (every other day)\n- Linkedin: networking (weekly)\n- Audible: listening to audiobooks (every other day)\n- Netflix: watching TV (every other day)\n- Spotify: listening to music (daily)\n\n## The next year\n\nThank god, it's 2019.\n\n---\n\n## Appendix\n\nTo improve my reflection, I have used [YearCompass]().\nIt's a 20-page PDF with many questions about your past year that helps you to structure your self-reflection a lot.\n\nThis post was inspired by [Martin Thoma's new year blogs](https://martin-thoma.com/new-year-2019/)\nand [Mathias Ockenfels' first year at Speedinvest X](https://medium.com/speedinvest/our-first-year-ef1558dc081e).\nThank you so much.\nI later found out that [Tim Ferris is proposing to do a year-review instad of new year resolutions](https://tim.blog/2018/12/28/past-year-review/).\n","folderName":"2018-year-in-review"},{"title":"Why every founder should use a notebook","slug":"reasons-to-use-a-notebook-as-a-founder","date":"2018-12-28T09:30:13.000Z","tags":"Notebook, Organization, Entrepreneurship","category":"Entrepreneurship","link":null,"description":"How I use mine, the benefits, and recommendations for getting started.","type":"text","content":"\nFor some time I have been using a notebook now.\nIt's simple, analogue, and convenient nonetheless.\nI first noticed people using one in university, when we heard talks from founders at our university club.\nAt that time, I bought one myself to try it out and have never looked back.\nAnd I think every founder should use one.\n\nIn this article I would like to articulate my love for notebooks.\nAnd I would like to help you get started using one.\nFirst, by talking about my usage.\nSecondly, by summarizing the benefits.\nAnd lastly, by giving some recommendations on which notebook to choose and how to structure it.\n\n## How I use my notebook\n\nI basically carry my notebook with me all the time.\nAnywhere.\nAs soon as something comes to my mind: I write it down.\nFor every meeting: I create a meeting note.\nFor every call: I write a note.\nFor every task that comes to my mind: I add a task to my ToDo note.\nFor every thought or self-reflection: I make a note.\nAs a result, if I need to recall something, I can just look it up.\nNo more searching across different places.\nAnd since everything is chronological, you find things quickly.\n\n## My top 5 reasons to use a notebook\n\nSo here are my top reasons to use a notebook in order of importance,\nprobably biased by my viewpoint from founder/investor perspective.\n\n### 1. Compendium Character\n\nThe most important reasons for me is that all of my notes are in one place.\nThis has two significant advantages.\nOne being that you do not have to switch between tools and write everything down in one notebook.\nThe other being that you also don't have to search in different places.\nWhile digital solutions offer this too, nothing keeps your notes together like one notebook.\n\n### 2. Portable anywhere\n\nAnother important reason for an analogue solution and thus a notebook is that you can carry a notebook anywhere.\nSure, you can take your phone anywhere, too.\nBut what about meetings, dinners, long travels, vacations, or nature?\nA notebook needs no power, no internet connection and is always available.\nI think the major advantage over a phone or tablet is that nothing will distract you when using it.\nAnd if everything else has an empty battery, your notebook is still there.\n\n### 3. Unobtrusive usage\n\nBuilding upon the last point, I would like to emphasize that analogue solutions still are more broadly accepted by society.\nI found this to be especially useful for business meetings or lunches where the usage of a smartphone or tablet would be frowned upon.\nOn the contrary, writing in your notebook shows that you take the other person seriously and consider their advice.\n\n### 4. One-handed sketching and note-taking\n\nAnother big advantage for me is that handwriting is the fastest way (known to me) to take one-haded notes or make sketches.\nBut why is this important?\nWhile people might be able to type faster with a keyboard or even with a phone,\nwhen I take notes I often include sketches, layouts, or even drawings.\nThe benefit of handwritten notes and a notebook is that you can do it one-handed,\nfor example while being on the phone.\n\n### 5. Digitizable later\n\nWhile many digital solutions to take notes exist,\nto me, none have all of the aforementioned benefits.\nSo if you still want to use notes in a digital format later-on,\nyou can always digitize specific pages or the whole book if you like.\n\n## My Recommendations\n\nFinally, some recommandations on structuring and choosing your notebook.\n\n### Structuring your notebook\n\nTo dive deeper into the topic, I can highly recommend the [bullet journal method](https://bulletjournal.com/pages/learn)\nas a starting point to structure your thoughts.\nFirst and foremost, simply make it a habit to take your notebook with you all the time.\nThe rest will follow automatically by using it.\n\n### Choosing a notebook\n\nIf you're thinking about buying a notebook, I would recommend the following:\nMy go-to format would be something close to DinA5 as is very portable and not too bulky.\nKeep in mind that you need to choose a smaller format if you want to carry it around in your pocket.\nFor the layout, I prefer a dotted layout over a squared one,\nas it gives you equally good orientation but more freedom.\nThus, I think the most popular choice would be a [Moleskine](http://a.co/d/6W1PsBp).\nPersonally, I prefer the [Lechtturm1917](http://a.co/d/cTNSIV5) version though.\nThe design is solid and very similar to a Moleskine, but it offers two significant benefits:\nFirstly, the pages are numbered, which is essential for me for later reference.\nSecondly, it comes with two bookmarks instead of one.\nAlso make sure to use the same pen everytime, it makes writing so much simpler.\n\nIf you have any questions, additions, comments, thoughts, let me know!\nHave fun with your notebook.\n\nFurther reading:\n\n- [HN: How do you keep track of your creative thoughts?](https://news.ycombinator.com/item?id=18837345)\n- If you are into digital solutions, I can highly recommend [notion.so](https://notion.so) and some [advanced Notion.so setups](https://www.youtube.com/watch?v=w_mh91IRLL8)\n","folderName":"2018-reasons-to-use-a-notebook"}]},"__N_SSG":true}